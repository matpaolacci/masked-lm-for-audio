{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpaolacci/masked-lm-for-audio/blob/main/vq_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ptV0pF7AsR"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE7NsKI9fXB9"
      },
      "source": [
        "## Set the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCCiwYjl7DUC"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "# Paths\n",
        "DATASET_HOME_DIR = '/content/drive/MyDrive/DLProj/Dataset'\n",
        "RAW_DATASET_DIR = os.path.join(DATASET_HOME_DIR, \"OriginalVersion\")        # the Slackh2100 dataset\n",
        "WAV_DATASET_DIR = os.path.join(DATASET_HOME_DIR, \"WavVersion\")             # the Slackh2100 dataset with .wav stems\n",
        "UTILITIES_DIR = '/content/drive/MyDrive/DLProj/Utilities'\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/DLProj/Checkpoints'\n",
        "\n",
        "sys.path.append(UTILITIES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfkiBYmLTFh"
      },
      "source": [
        "## Set Configuration variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf4dyxBfLZM7"
      },
      "outputs": [],
      "source": [
        "# Set this to True if you want create the dataset\n",
        "PREPARE_DATASET = False\n",
        "USE_STEMS = False # if true we use stems to train vqvae else use mixes tracks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdRVdFXVLc3t"
      },
      "source": [
        "### Use pre-trained model\n",
        "If you want continue the training from previously saved model compile the following section by declaring the path to the model along with the path of used hyperparametes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBa1VjI0L6k-"
      },
      "outputs": [],
      "source": [
        "USE_PRE_SAVED_MODEL = False\n",
        "\n",
        "# Set following fields only if previous variable has been set to True\n",
        "model_checkpoints_dir = os.path.join(CHECKPOINTS_DIR, '2024-09-14_123325')\n",
        "PATH_TO_PRE_SAVED_MODEL = os.path.join(model_checkpoints_dir, 'VQVAE_19999_EPOCH')\n",
        "\n",
        "PATH_TO_HYPERPARAMETERS = os.path.join(model_checkpoints_dir, 'hyper_params.json')\n",
        "\n",
        "# Set for how many epochs the presaved model will be trained\n",
        "EPOCHS_RE_TRAIN = 3000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bisNjT_dwgr"
      },
      "source": [
        "### Use pre-trained model for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM3HSFlMd13c"
      },
      "outputs": [],
      "source": [
        "ONLY_EVALUATION = False\n",
        "EVAL_CHECKPOINT_DIR = os.path.join(CHECKPOINTS_DIR, \"2024-09-14_180502\")\n",
        "PATH_TO_MODEL_TO_EVALUATE = os.path.join(EVAL_CHECKPOINT_DIR, \"VQVAE_19979_EPOCH\")\n",
        "PATH_TO_HYPERPARAMETERS_EVALUATION = os.path.join(EVAL_CHECKPOINT_DIR, 'hyper_params.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMMLCvIl63zA"
      },
      "source": [
        "## Install the required python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Iqr2ZDK74lO"
      },
      "outputs": [],
      "source": [
        "!pip install -r $UTILITIES_DIR/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95647-9tfSZs"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRhSFMPoe0m-"
      },
      "outputs": [],
      "source": [
        "import zipfile, yaml\n",
        "import flacconverter as fc\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "import gc\n",
        "import json\n",
        "\n",
        "import torch, torchaudio, pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# For getting data structure from list\n",
        "import ast\n",
        "\n",
        "# Libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# To plot the graphs of loss and perplexity\n",
        "from scipy.signal import savgol_filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jSmlnitmvb1"
      },
      "source": [
        "## Create checkpoints directory\n",
        "It will be created a directory for each training, each one will contain the best trained model across the epochs and a file containing the used hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRYy2eWYnRNn"
      },
      "outputs": [],
      "source": [
        "datetime_start = (datetime.now() + timedelta(hours=2)).strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "CURR_CHECKPOINT_DIR = os.path.join(CHECKPOINTS_DIR, datetime_start)\n",
        "if not ONLY_EVALUATION:\n",
        "    os.makedirs(CURR_CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ob4w7d2riwU"
      },
      "source": [
        "## Set log file and log function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrUaQfDfrko1"
      },
      "outputs": [],
      "source": [
        "data = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "LOG_FILE_PATH = os.path.join(CURR_CHECKPOINT_DIR, f'{data}.log')\n",
        "\n",
        "def log(message: str):\n",
        "    if ONLY_EVALUATION:\n",
        "        return\n",
        "\n",
        "    with open(LOG_FILE_PATH, \"a\") as log_file:\n",
        "        now = datetime.now()\n",
        "        now_local = now + timedelta(hours=2)\n",
        "        log_file.write(f'[{now_local.strftime(\"%Y-%m-%d %H:%M:%S\")}] - {message}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeifX3VCsl63"
      },
      "source": [
        "# Prepare the data\n",
        "In this section we are going to download the Slakh2100 dataset available [here](http://www.slakh.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfA08phcnqVU"
      },
      "outputs": [],
      "source": [
        "class Instrument(Enum):\n",
        "    BASS = 'Bass'\n",
        "    BRASS = 'Brass'\n",
        "    CHROMATIC_PERCUSSION = 'Chromatic Percussion'\n",
        "    DRUMS = 'Drums'\n",
        "    ETHNIC = 'Ethnic'\n",
        "    GUITAR = 'Guitar'\n",
        "    ORGAN = 'Organ'\n",
        "    PERCUSSIVE = 'Percussive'\n",
        "    PIANO = 'Piano'\n",
        "    PIPE = 'Pipe'\n",
        "    REED = 'Reed'\n",
        "    SOUND_EFFECTS = 'Sound Effects'\n",
        "    STRINGS = 'Strings'\n",
        "    STRINGS_CONTINUED = 'Strings (continued)'\n",
        "    SYNTH_LEAD = 'Synth Lead'\n",
        "    SYNTH_PAD = 'Synth Pad'\n",
        "\n",
        "def extractZipDataset():\n",
        "    # Apri il file ZIP e estrai i contenuti\n",
        "    with zipfile.ZipFile(os.path.join(RAW_DATASET_DIR, 'dataset.zip'), 'r') as zipRef:\n",
        "        zipRef.extractall(RAW_DATASET_DIR)\n",
        "\n",
        "    print(f'File estratti in {RAW_DATASET_DIR}')\n",
        "\n",
        "def convertToWav(baseDir: str, outDir: str):\n",
        "    fc.to_wav(baseDir, outDir, n_threads=2)\n",
        "\n",
        "def createDatasetForVqVae(instruments_set: set[str], split_sets = ['train', 'test', 'validation']):\n",
        "    \"\"\"Questa funzione deve generare un file csv contenente i path, uno per riga, delle tracce audio composte\n",
        "    dagli strumenti nell'instrumentSet.\n",
        "    \"\"\"\n",
        "\n",
        "    for set_dir in split_sets:\n",
        "        csv_stems_file_path = os.path.join(DATASET_HOME_DIR, set_dir + \"_stems.csv\")\n",
        "        csv_mixes_file_path = os.path.join(DATASET_HOME_DIR, set_dir + \"_mixes.csv\")\n",
        "\n",
        "        if os.path.exists(csv_stems_file_path):\n",
        "            print(f\"The file \\'{csv_stems_file_path}\\' already exists! Are you sure you want overwrite it?\")\n",
        "            continue\n",
        "\n",
        "        elif os.path.exists(csv_mixes_file_path):\n",
        "            print(f\"The file \\'{csv_mixes_file_path}\\' already exists! Are you sure you want overwrite it?\")\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            # Adding the table header to the csv for stems\n",
        "            with open(csv_stems_file_path, 'w') as csv_stems_file:\n",
        "                csv_stems_file.write(\"file_path;instrument_class;midi_program;track_name\\n\")\n",
        "\n",
        "            # Adding the table header to the csv for mixes\n",
        "            with open(csv_mixes_file_path, 'w') as csv_mixes_file:\n",
        "                csv_mixes_file.write(\"file_path;midi_programs;instruments_classes;track_name\\n\")\n",
        "\n",
        "            # Just creating the label to be added to tqdm\n",
        "            tqdm_description = f\"Creating {os.path.basename(csv_stems_file_path)} and {os.path.basename(csv_mixes_file_path)} files\"\n",
        "\n",
        "            # Here we keep track of the file doesn't exist\n",
        "            not_existing_stems = []\n",
        "\n",
        "            for track_dir_name in tqdm(os.listdir(os.path.join(WAV_DATASET_DIR, set_dir)), desc=tqdm_description):\n",
        "                if not os.path.isdir(os.path.join(WAV_DATASET_DIR, set_dir, track_dir_name)):\n",
        "                    continue\n",
        "\n",
        "                # Compose the path to the directory of the track\n",
        "                track_dir_path = os.path.join(WAV_DATASET_DIR, set_dir, track_dir_name)\n",
        "\n",
        "                with open(os.path.join(track_dir_path, \"metadata.yaml\"), 'r') as file:\n",
        "                    yamldata = yaml.safe_load(file)\n",
        "\n",
        "                    # The list of the midi programs that composed the track\n",
        "                    midi_programs_list: list[str] = []\n",
        "\n",
        "                    # The set of the instrument class (es. guitar, piano) that composed the track\n",
        "                    #   we need of a set since a track can be composed by several midi programs\n",
        "                    #   (es. Electic Guitar, Classic Guitar) belonging to the same instrument class\n",
        "                    instruments_classes_set: set[str] = set()\n",
        "\n",
        "                    for stem_name in yamldata[\"stems\"]:\n",
        "                        # get the name of the instruments\n",
        "                        instr_class: str = yamldata[\"stems\"][stem_name][\"inst_class\"]\n",
        "                        midi_program: str = yamldata[\"stems\"][stem_name][\"midi_program_name\"]\n",
        "                        midi_programs_list.append(midi_program)\n",
        "                        instruments_classes_set.add(instr_class)\n",
        "\n",
        "                        if instr_class in instruments_set:\n",
        "\n",
        "                            # The path to the stem file\n",
        "                            stem_file_path = os.path.join(track_dir_path, 'stems', stem_name) + \".wav\"\n",
        "\n",
        "                            # check if the file exists\n",
        "                            if not os.path.exists(stem_file_path):\n",
        "                                not_existing_stems.append(stem_file_path)\n",
        "                                continue\n",
        "\n",
        "                            # add the path to the stem of the intrument to the csv\n",
        "                            entry = f\"{stem_file_path};{instr_class};{midi_program};{track_dir_name.lower()}\\n\"\n",
        "\n",
        "                            with open(csv_stems_file_path, 'a') as csv_stems_file:\n",
        "                                csv_stems_file.write(entry)\n",
        "\n",
        "                    mixes_file_path = os.path.join(track_dir_path, 'mix.wav')\n",
        "\n",
        "                    if not os.path.exists(mixes_file_path):\n",
        "                        not_existing_stems.append(mixes_file_path)\n",
        "                        continue\n",
        "\n",
        "                    with open(csv_mixes_file_path, 'a') as csv_mixes_file:\n",
        "                        entry = f\"{mixes_file_path};{midi_programs_list};{list(instruments_classes_set)};{track_dir_name}\\n\"\n",
        "                        csv_mixes_file.write(entry)\n",
        "\n",
        "            # Print the errors\n",
        "            print(f\"The following files don't exist in the {set_dir} set:\")\n",
        "            for err in not_existing_stems:\n",
        "                print(f\"{' ' * 4}{err}\")\n",
        "\n",
        "\n",
        "def prepareDataset():\n",
        "    extractZipDataset()\n",
        "\n",
        "    # Convert each set to wav\n",
        "    for dir in ['train', 'test', 'validation']:\n",
        "        convertToWav(os.path.join(RAW_DATASET_DIR, dir), os.path.join(WAV_DATASET_DIR, dir))\n",
        "\n",
        "    createDatasetForVqVae({Instrument.BASS.value, Instrument.PIANO.value, Instrument.DRUMS.value, Instrument.GUITAR.value})\n",
        "\n",
        "    print(\"Dataset is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkG5az2zCbeU"
      },
      "outputs": [],
      "source": [
        "#createDatasetForVqVae({Instrument.BASS.value, Instrument.PIANO.value, Instrument.DRUMS.value, Instrument.GUITAR.value} )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt_FMAUjQvuf"
      },
      "source": [
        "# Look at the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGwXJnh1jT5k"
      },
      "source": [
        "## Composition of the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2OSkkqIjmHb"
      },
      "outputs": [],
      "source": [
        "FIG_SIZE = (15, 7)\n",
        "\n",
        "pathToHistogramTrain = os.path.join(RAW_DATASET_DIR, \"histograms/train.png\")\n",
        "pathToHistogramTest = os.path.join(RAW_DATASET_DIR, \"histograms/test.png\")\n",
        "pathToHistogramValidation = os.path.join(RAW_DATASET_DIR, \"histograms/validation.png\")\n",
        "\n",
        "trainHist = mpimg.imread(pathToHistogramTrain)\n",
        "plt.figure(figsize=FIG_SIZE)\n",
        "plt.imshow(trainHist)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0F-8sxCb0-t"
      },
      "source": [
        "## Let's Plot the graphic of some tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAohKfpYy0mD"
      },
      "outputs": [],
      "source": [
        "def plot_waveform(waveform, graph_title: str, from_sec:int = None, to_sec: int = None, sample_rate = 44100):\n",
        "    if to_sec is not None:\n",
        "        waveform = waveform[:, :to_sec*sample_rate]\n",
        "\n",
        "    if from_sec is not None:\n",
        "        waveform = waveform[:, from_sec*sample_rate:]\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(from_sec if from_sec is not None else 0, num_frames + (from_sec if from_sec is not None else 0)) / sample_rate\n",
        "\n",
        "    # Crea il plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        plt.plot(time_axis.numpy(), waveform[i].numpy(), label=f'Channel {i+1}')\n",
        "\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title(graph_title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnrU0Vjk9rkt"
      },
      "source": [
        "# Define the Pytorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zQkXV3eSd2"
      },
      "outputs": [],
      "source": [
        "# The header of the csv are\n",
        "#   *_stems.csv -> ['file_path', 'instrument_class', 'midi_program', 'track_name']\n",
        "#   *_mixes.csv -> ['file_path', 'midi_programs', 'instruments_classes', 'track_name']\n",
        "\n",
        "#TODO: da sistemare\n",
        "SAMPLE_RATE = 44100 #Hz\n",
        "\n",
        "def get_alert_message(path, audio_length):\n",
        "    return f\"The file \\\"{path}\\\" is shorter than {audio_length}sec\"\n",
        "\n",
        "class StemsDataset(Dataset):\n",
        "    def __init__(self, csv_audio_path, audio_length, frac_dataset=None, batch_size=None, transform=None):\n",
        "        self.audio_csv = pd.read_csv(csv_audio_path, delimiter=';')\n",
        "        if(frac_dataset != None):\n",
        "            assert frac_dataset<=1.0 and frac_dataset>0\n",
        "            sample_size = max(1, int(len(self.audio_csv) * frac_dataset))\n",
        "            self.audio_csv = self.audio_csv.sample(n=sample_size, random_state=1)\n",
        "\n",
        "        self.audio_length = audio_length\n",
        "        self.transform = transform\n",
        "\n",
        "        # In this way we ensure the last batch have the right size\n",
        "        if(batch_size is not None):\n",
        "            self.audio_csv = self.audio_csv.head(len(self.audio_csv) - (len(self.audio_csv) % batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_csv.iloc[idx, 0]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        instrument_class = self.audio_csv.iloc[idx, 1]\n",
        "        midi_program = self.audio_csv.iloc[idx, 2]\n",
        "\n",
        "        assert waveform.shape[1] >= SAMPLE_RATE * self.audio_length, get_alert_message(audio_path, self.audio_length)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        assert waveform.shape[1]/SAMPLE_RATE >= self.audio_length, f\"The transformed audio is too short ({waveform.shape[1]/SAMPLE_RATE}sec)!\"\n",
        "\n",
        "        # take the first audio_length seconds\n",
        "        waveform = waveform[:,0: SAMPLE_RATE * self.audio_length]\n",
        "\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kztVvJKTw_V6"
      },
      "outputs": [],
      "source": [
        "class MixesDataset(Dataset):\n",
        "    def __init__(self, csv_audio_path, audio_length, frac_dataset=None, batch_size=None, transform=None):\n",
        "        self.audio_csv = pd.read_csv(csv_audio_path, delimiter=';')\n",
        "        if(frac_dataset != None):\n",
        "            assert frac_dataset<=1.0 and frac_dataset>0\n",
        "            sample_size = max(1, int(len(self.audio_csv) * frac_dataset))\n",
        "            self.audio_csv = self.audio_csv.sample(n=sample_size, random_state=1)\n",
        "\n",
        "        self.audio_length = audio_length\n",
        "        self.transform = transform\n",
        "\n",
        "        # In this way we ensure the last batch have the right size\n",
        "        if(batch_size is not None):\n",
        "            self.audio_csv = self.audio_csv.head(len(self.audio_csv) - (len(self.audio_csv) % batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_csv.iloc[idx, 0]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        midi_programs = ast.literal_eval(self.audio_csv.iloc[idx, 1])\n",
        "        instruments_class = ast.literal_eval(self.audio_csv.iloc[idx, 2])\n",
        "        track_name = self.audio_csv.iloc[idx, 3]\n",
        "\n",
        "        assert waveform.shape[1] >= SAMPLE_RATE * self.audio_length, get_alert_message(audio_path, self.audio_length)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        assert waveform.shape[1]/SAMPLE_RATE >= self.audio_length, f\"The transformed audio is too short ({waveform.shape[1]/SAMPLE_RATE}sec)!\"\n",
        "\n",
        "        # take the first audio_length seconds\n",
        "        waveform = waveform[:,0: SAMPLE_RATE * self.audio_length]\n",
        "\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZgl8MDuUm2"
      },
      "source": [
        "# VQ-VAE: The model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4y1dg5ycN9N"
      },
      "source": [
        "## The Vector Quantizer module\n",
        "We calculate the loss by summing up two terms:\n",
        "- **codebook loss**, which moves the embedding towards the encoder output;\n",
        "- **commitment loss**, which makes sure the encoder commits to an embedding;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim) # TODO: Verifica con float64\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCW -> BWC\n",
        "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # # Calculate L2-normalized distance between the inputs and the codes\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)                  # a^2\n",
        "                    + torch.sum(self._embedding.weight.t()**2, dim=0, keepdim=True) # b^2\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))     # -2ab\n",
        "\n",
        "        # Get the index of the neareast code (from codebook) for each embeddings output by encoder\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = self._embedding(encoding_indices).view(input_shape)\n",
        "\n",
        "        # Calculating commitment loss and codebook loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # commitment loss\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach()) # codebook loss\n",
        "        loss = torch.exp(q_latent_loss) + torch.exp(self._commitment_cost * e_latent_loss)\n",
        "\n",
        "        # Straight-through estimator trick for gradient backpropagation\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        # Create a one-hot matrix where the ones at (i, j) position\n",
        "        #   indicates that the j-th code, from codebook, it was selected\n",
        "        #   for the i-th embedding vector output by the encoder.\n",
        "        one_hot_enc = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        one_hot_enc.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Calculates the average probability of the utilization of each code from codebook\n",
        "        avg_probs = torch.mean(one_hot_enc, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BWC -> BCHW\n",
        "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity, quantized\n"
      ],
      "metadata": {
        "id": "9vnH_2r3NCPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        self._commitment_cost = commitment_cost\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim) # TODO: Verifica con float64\n",
        "        self._embedding.weight.data.normal_()\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCW -> BWC\n",
        "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # # Calculate L2-normalized distance between the inputs and the codes\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)                  # a^2\n",
        "                    + torch.sum(self._embedding.weight.t()**2, dim=0, keepdim=True) # b^2\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))     # -2ab\n",
        "\n",
        "        # Get the index of the neareast code (from codebook) for each embeddings output by encoder\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = self._embedding(encoding_indices).view(input_shape)\n",
        "\n",
        "        # Create a one-hot matrix where the ones at (i, j) position\n",
        "        #   indicates that the j-th code, from codebook, it was selected\n",
        "        #   for the i-th embedding vector output by the encoder.\n",
        "        one_hot_enc = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        one_hot_enc.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Use EMA to update the embedding vectors (codebook)\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(one_hot_enc, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(one_hot_enc.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Calculating commitment loss and codebook loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # commitment loss\n",
        "        loss = torch.exp(self._commitment_cost * e_latent_loss)\n",
        "\n",
        "        # Straight-through estimator trick for gradient backpropagation\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        # Calculates the average probability of the utilization of each code from codebook\n",
        "        avg_probs = torch.mean(one_hot_enc, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BWC -> BCHW\n",
        "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity, quantized"
      ],
      "metadata": {
        "id": "wIRG93RwJshe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAI9PUCecC7b"
      },
      "source": [
        "## The ResNet module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f7-9JkMb_1j"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLL6iEoJgsL"
      },
      "source": [
        "## Some utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRyQmEzUJlXZ"
      },
      "outputs": [],
      "source": [
        "from math import log2\n",
        "\n",
        "def get_levels(end, num_layers, reverse=False):\n",
        "    '''This function returns a list of 2-ple (input_channels, output_channels) representing the\n",
        "    input and output dimensions of each encoder/decoder block.\n",
        "    Each two subsequent layer it will be a halving/doubling of the dimensionality of the output channels.\n",
        "\n",
        "    For example, for end=128 and num_layers=4,\n",
        "    it will return:\n",
        "            1 -> 16\n",
        "            16 -> 32\n",
        "            32 -> 64\n",
        "            64 -> 128\n",
        "    '''\n",
        "    input_hiddens = []\n",
        "    output_hiddens = []\n",
        "\n",
        "    if (log2(end) - (int(log2(end))) > 0):\n",
        "        raise RuntimeError(f\"{end} isn't power of 2\")\n",
        "    elif (log2(end) < num_layers):\n",
        "        raise RuntimeError(f\"num_layers ({num_layers}) is greater than the number of powers of 2 ({int(log2(end))}) for {end}\")\n",
        "\n",
        "    while(end>=2 and num_layers>0):\n",
        "        last = not (end//2>=2 and num_layers-1>0)\n",
        "        input_hiddens.append(end//2 if not last else 1)\n",
        "        output_hiddens.append(end)\n",
        "\n",
        "        # Update the loop parameters\n",
        "        end = end//2\n",
        "        num_layers-=1\n",
        "\n",
        "    input_hiddens, output_hiddens = input_hiddens[::-1 if not reverse else None], output_hiddens[::-1 if not reverse else None]\n",
        "\n",
        "    if reverse:\n",
        "        return zip(output_hiddens, input_hiddens)\n",
        "    else:\n",
        "        return zip(input_hiddens, output_hiddens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muXre5Tkg6G7"
      },
      "source": [
        "## The Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM4HenS-bZ8q"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation_function):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self._activation_function = activation_function\n",
        "        self._conv = nn.Conv1d( in_channels=in_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=4,\n",
        "                                stride=2,\n",
        "                                padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._activation_function(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_halving_layers, num_residual_layers, num_residual_hiddens):\n",
        "        '''The encoder takes a in_channels-dimensional embedding sequence of length N as input\n",
        "        and maps it in a sequence N/\"2^num_halving_layers\" smaller of embeddings of dimension \"num_hiddens\".\n",
        "        '''\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self._block_activation_function = nn.ReLU()\n",
        "\n",
        "        self._layers = [\n",
        "            EncoderBlock(in_channels=_in_channels,\n",
        "                         out_channels=_out_channels,\n",
        "                         activation_function=nn.Identity() if (_out_channels == num_hiddens) else self._block_activation_function\n",
        "            )\n",
        "            for _in_channels, _out_channels in get_levels(num_hiddens, num_halving_layers)\n",
        "        ] + [\n",
        "            nn.Conv1d(  in_channels=num_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "\n",
        "            ResidualStack(  in_channels=num_hiddens,\n",
        "                            num_hiddens=num_hiddens,\n",
        "                            num_residual_layers=num_residual_layers,\n",
        "                            num_residual_hiddens=num_residual_hiddens),\n",
        "        ]\n",
        "\n",
        "        self._encoder = nn.Sequential(*self._layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self._encoder(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2r0n2fmcj7F"
      },
      "source": [
        "## The Decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmOdjEuobXMG"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation_function):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self._conv = nn.ConvTranspose1d(in_channels=in_channels,\n",
        "                                        out_channels=out_channels,\n",
        "                                        kernel_size=4,\n",
        "                                        stride=2,\n",
        "                                        padding=1)\n",
        "\n",
        "        self._activation_function = activation_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._activation_function(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_doubling_layer, num_residual_layers, num_residual_hiddens):\n",
        "        '''The decoder takes a in_channels-dimensional embedding sequence of length N as input\n",
        "        and maps it in a sequence N*\"2^num_doubling_layer\" longer of embeddings of dimension \"num_hiddens\".\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._block_activation_function = nn.ReLU()\n",
        "        self._output_activation_func = nn.Tanh()  # Output values in range [-1, 1]\n",
        "\n",
        "        self._layers = [\n",
        "            nn.Conv1d(  in_channels=num_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "\n",
        "            ResidualStack(in_channels=num_hiddens,\n",
        "                        num_hiddens=num_hiddens,\n",
        "                        num_residual_layers=num_residual_layers,\n",
        "                        num_residual_hiddens=num_residual_hiddens)\n",
        "        ] + [\n",
        "            DecoderBlock(in_channels=_in_channels,\n",
        "                        out_channels=_out_channels,\n",
        "                        activation_function=nn.Identity() if (_out_channels == 1) else self._block_activation_function)\n",
        "            for _in_channels, _out_channels in get_levels(num_hiddens, num_doubling_layer, reverse=True)\n",
        "        ]\n",
        "\n",
        "        self._decoder = nn.Sequential(*self._layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._decoder(x)\n",
        "        return self._output_activation_func(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ih6i1JiXfy"
      },
      "source": [
        "## The VQ-VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0YzSnOLibvH"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_halving_layers, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay):\n",
        "\n",
        "        super(VQVAE, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(1,\n",
        "                                num_hiddens,\n",
        "                                num_halving_layers,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "        # It maps the \"num_hiddens-dimensional\" embedding sequence in a \"embedding_dim-dimensional\" sequence\n",
        "        #   preparing it for quantization\n",
        "        self._pre_vq_conv = nn.Conv1d(in_channels=num_hiddens,\n",
        "                                      out_channels=embedding_dim,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1)\n",
        "\n",
        "        if decay > 0.0:\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                               commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
        "                                            commitment_cost)\n",
        "\n",
        "        # It maps the \"embedding_dim-dimensional\" embedding sequence in a \"num_hiddens-dimensional\" embeddin sequence\n",
        "        #   preparing it for the decoding operation (added by me)\n",
        "        self._post_vq_conv = nn.Conv1d(in_channels=embedding_dim,\n",
        "                                       out_channels=num_hiddens,\n",
        "                                       kernel_size=1,\n",
        "                                       stride=1)\n",
        "\n",
        "        self._decoder = Decoder(num_hiddens,\n",
        "                                num_hiddens,\n",
        "                                num_halving_layers,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x)\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        decoder_input = self._post_vq_conv(quantized)\n",
        "        x_recon = self._decoder(decoder_input)\n",
        "\n",
        "        return loss, x_recon, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k_rJNWZqyVZ"
      },
      "source": [
        "# Training of the VQ-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ea4LVlZOT_k"
      },
      "source": [
        "## Select the device (cpu or gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU4dGQO7hyFH"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"You are using {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CO8vSPBqz90"
      },
      "source": [
        "## Define the hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nwcai9uq29e"
      },
      "outputs": [],
      "source": [
        "if USE_PRE_SAVED_MODEL:\n",
        "    with open(PATH_TO_HYPERPARAMETERS, 'r') as hyper_params_file:\n",
        "        HYPER_PARAMS = json.load(hyper_params_file)\n",
        "        HYPER_PARAMS[\"epochs\"] = EPOCHS_RE_TRAIN\n",
        "\n",
        "elif ONLY_EVALUATION:\n",
        "    with open(PATH_TO_HYPERPARAMETERS_EVALUATION, 'r') as hyper_params_file:\n",
        "        HYPER_PARAMS = json.load(hyper_params_file)\n",
        "else:\n",
        "    HYPER_PARAMS = {\n",
        "        \"batch_size\": 10,\n",
        "        \"dataset_frac_size\": 0.40,\n",
        "        \"epochs\": 20000,\n",
        "\n",
        "        # Inputs audio track length in seconds\n",
        "        \"audio_length\": 16,\n",
        "\n",
        "        # Control the silence in the track\n",
        "        \"remove_silence\": True,\n",
        "\n",
        "        # Dimensionality of each encoded vector before quantization\n",
        "        \"num_hiddens\": 128,\n",
        "        \"num_halving_layers\": 6,\n",
        "\n",
        "        # The ResNet parameters\n",
        "        \"num_residual_hiddens\": 64,\n",
        "        \"num_residual_layers\": 3,\n",
        "\n",
        "        # The dimensionality of the space where the codebook lies\n",
        "        \"embedding_dim\": 64,\n",
        "\n",
        "        # Number of the codes in the codebook\n",
        "        \"num_embeddings\": 512,\n",
        "\n",
        "        # To make sure the encoder commits to an embedding\n",
        "        \"commitment_cost\": 1.0,\n",
        "        \"decay\": 0.99,\n",
        "        \"learning_rate\": 0.0002,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV4BC4L5s6AK"
      },
      "source": [
        "Save the hyper params configuration in the training dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJSW1L6bsTaI"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    with open(os.path.join(CURR_CHECKPOINT_DIR, \"hyper_params.json\"), 'w') as hyp_params_file:\n",
        "        json.dump(HYPER_PARAMS, hyp_params_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H0rLSXKDzvi"
      },
      "source": [
        "## Create the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jwtl8xmF7E"
      },
      "source": [
        "### Define the transformation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-dPKsbimFZD"
      },
      "outputs": [],
      "source": [
        "def trim_silence(waveform: torch.Tensor):\n",
        "    return waveform[waveform != 0].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRbm1kUumOjS"
      },
      "source": [
        "### Build the Pytorch dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FiDaYxjD4O-"
      },
      "outputs": [],
      "source": [
        "if USE_STEMS:\n",
        "    train_set = StemsDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"train_stems.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    batch_size=HYPER_PARAMS['batch_size'],\n",
        "                    transform=trim_silence if HYPER_PARAMS['remove_silence'] else None\n",
        "    )\n",
        "    test_set = StemsDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"test_stems.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "    )\n",
        "    validation_set = StemsDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"validation_stems.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    batch_size=HYPER_PARAMS['batch_size']\n",
        "    )\n",
        "else:\n",
        "    train_set = MixesDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"train_mixes.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    frac_dataset=HYPER_PARAMS['dataset_frac_size']\n",
        "    )\n",
        "    test_set = MixesDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"test_mixes.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    frac_dataset=HYPER_PARAMS['dataset_frac_size']\n",
        "    )\n",
        "    validation_set = MixesDataset(\n",
        "                    os.path.join(DATASET_HOME_DIR, \"validation_mixes.csv\"),\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    frac_dataset=HYPER_PARAMS['dataset_frac_size']\n",
        "    )\n",
        "\n",
        "print(f\"Train stems len: {len(train_set)}\")\n",
        "print(f\"Test stems len: {len(test_set)}\")\n",
        "print(f\"Validation stems len: {len(validation_set)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3A1KlF-tbh"
      },
      "source": [
        "## Create the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra9-Cqux-v0n"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    train_vqvae_dataloader = DataLoader(train_set, batch_size=HYPER_PARAMS['batch_size'], shuffle=True)\n",
        "    validation_vqvae_dataloader = DataLoader(validation_set, batch_size=HYPER_PARAMS['batch_size'], shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVAFttGfNOVB"
      },
      "source": [
        "## Configure the model to train\n",
        "If we set the 'USE_PRE_SAVED_MODEL' to *True* then the selected pre-saved model will be train, else a new one will be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROGanJrNUTE"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    # define the model\n",
        "    model = VQVAE(\n",
        "        HYPER_PARAMS[\"num_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_halving_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_embeddings\"],\n",
        "        HYPER_PARAMS[\"embedding_dim\"],\n",
        "        HYPER_PARAMS[\"commitment_cost\"],\n",
        "        HYPER_PARAMS[\"decay\"]\n",
        "    ).to(device)\n",
        "\n",
        "    if USE_PRE_SAVED_MODEL:\n",
        "        model.load_state_dict(\n",
        "            torch.load(PATH_TO_PRE_SAVED_MODEL, map_location=torch.device('cuda'))\n",
        "        )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=HYPER_PARAMS['learning_rate'], amsgrad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYPuEYXN_Ne8"
      },
      "source": [
        "## Define a per-epoch training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_yKjgsWezdz"
      },
      "outputs": [],
      "source": [
        "def plot_loss_and_perplexity(train_res_recon_error, train_res_perplexity):\n",
        "    WINDOW_LENGTH = 201\n",
        "\n",
        "    f = plt.figure(figsize=(16,8))\n",
        "    ax = f.add_subplot(1,2,1)\n",
        "    ax.plot(savgol_filter(train_res_recon_error, WINDOW_LENGTH, 7))\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_title('Smoothed NMSE.')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    ax = f.add_subplot(1,2,2)\n",
        "    ax.plot(savgol_filter(train_res_perplexity, WINDOW_LENGTH, 7))\n",
        "    ax.set_title('Smoothed Average codebook usage (perplexity).')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    f.savefig(os.path.join(EVAL_CHECKPOINT_DIR if ONLY_EVALUATION else CURR_CHECKPOINT_DIR, \"loss_and_perplexity.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGf7Gsz4burt"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_dataloader: DataLoader, epoch_index, tb_writer):\n",
        "\n",
        "    train_res_recon_error = []\n",
        "    train_res_perplexity = []\n",
        "\n",
        "    for _, inputs in enumerate(train_dataloader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        data_variance = torch.var(inputs, correction=0)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        vq_loss, data_recon, perplexity = model(inputs)\n",
        "\n",
        "        # Compute the losses and its gradients\n",
        "        recon_error = F.mse_loss(data_recon, inputs) / data_variance\n",
        "        #vq_loss = vq_loss / data_variance\n",
        "        loss = recon_error + vq_loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        train_res_recon_error.append(recon_error.item())\n",
        "        train_res_perplexity.append(perplexity.item())\n",
        "\n",
        "    return np.mean(train_res_recon_error), np.mean(train_res_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpeRM4-A_Xt2"
      },
      "source": [
        "## Define the main training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcgDp1iDbt07"
      },
      "outputs": [],
      "source": [
        "KEEP_LAST_N_MODELS = 5\n",
        "THRESHOLD_ERROR_TO_SAVE_MODEL = 0.2\n",
        "\n",
        "def training_loop():\n",
        "\n",
        "    # Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "    timestamp = (datetime.now() + timedelta(hours=2)).strftime('%Y%m%d_%H%M%S')\n",
        "    writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
        "    epoch_number = 0\n",
        "\n",
        "    best_recon_error = 1_000_000\n",
        "    best_perplexity = 1_000_000\n",
        "\n",
        "    # To keep track of the saved models\n",
        "    saved_models = []\n",
        "\n",
        "    # To keep track of the loss and perplexity along epochs and finally plot the graphs\n",
        "    train_recon_error_per_epoch = []\n",
        "    train_perplexity_per_epoch = []\n",
        "\n",
        "    with tqdm(total=HYPER_PARAMS[\"epochs\"]) as pbar:\n",
        "        for epoch in range(HYPER_PARAMS[\"epochs\"]):\n",
        "            log('EPOCH {}'.format(epoch_number + 1))\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            model.train()\n",
        "            avg_train_recon_error, avg_train_perplexity = train_one_epoch(train_vqvae_dataloader,epoch_number, writer)\n",
        "\n",
        "            # Store statistics\n",
        "            train_recon_error_per_epoch.append(avg_train_recon_error)\n",
        "            train_perplexity_per_epoch.append(avg_train_perplexity)\n",
        "\n",
        "            # Set the model to evaluation mode, disabling dropout and using population\n",
        "            # statistics for batch normalization.\n",
        "            model.eval()\n",
        "\n",
        "            valid_res_recon_error = []\n",
        "            valid_res_perplexity = []\n",
        "\n",
        "            # Disable gradient computation and reduce memory consumption.\n",
        "            with torch.no_grad():\n",
        "                for i, valid_inputs in enumerate(validation_vqvae_dataloader):\n",
        "                    valid_inputs = valid_inputs.to(device)\n",
        "                    data_variance = torch.var(valid_inputs, correction=0)\n",
        "                    valid_vq_loss, valid_data_recon, valid_perplexity = model(valid_inputs)\n",
        "                    recon_error = F.mse_loss(valid_data_recon, valid_inputs) / data_variance\n",
        "                    valid_res_recon_error.append(recon_error.item())\n",
        "                    valid_res_perplexity.append(valid_perplexity.item())\n",
        "\n",
        "            avg_valid_recon_error = np.mean(valid_res_recon_error)\n",
        "            avg_valid_perplexity = np.mean(valid_res_perplexity)\n",
        "\n",
        "            # Log error and perplexity\n",
        "            log(f\"Train reconstruction error: {avg_train_recon_error} - Validation reconstruction error: {avg_valid_recon_error}\")\n",
        "            log(f\"Train perplexity: {avg_train_perplexity} - Validation perplexity: {avg_valid_perplexity}\")\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Train reconstruction err': f'{avg_train_recon_error:.5f}',\n",
        "                'Train perplexity': f'{avg_train_perplexity:.5f}'\n",
        "            })\n",
        "\n",
        "            # Log the running loss averaged per batch\n",
        "            # for both training and validation\n",
        "            writer.add_scalars('Training vs. Validation reconstruction Loss',\n",
        "                            { 'Training' : avg_train_recon_error, 'Validation' : avg_valid_recon_error },\n",
        "                            epoch_number + 1)\n",
        "            writer.flush()\n",
        "\n",
        "            # Track best performance, and save the model's state\n",
        "            if (avg_valid_recon_error < THRESHOLD_ERROR_TO_SAVE_MODEL) and (\n",
        "                avg_valid_recon_error < best_recon_error or avg_valid_perplexity < best_perplexity):\n",
        "\n",
        "                best_recon_error = avg_valid_recon_error\n",
        "                best_perplexity = avg_valid_perplexity\n",
        "\n",
        "                model_name = 'VQVAE_{}_EPOCH'.format(epoch_number + 1)\n",
        "                model_path = os.path.join(CURR_CHECKPOINT_DIR, model_name)\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "\n",
        "                # Keep track of saved models\n",
        "                saved_models.append(model_path)\n",
        "\n",
        "                # delete too old saved models\n",
        "                if len(saved_models) > KEEP_LAST_N_MODELS:\n",
        "                    os.remove(saved_models[0])\n",
        "                    saved_models = saved_models[1:]\n",
        "\n",
        "            epoch_number += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    plot_loss_and_perplexity(train_recon_error_per_epoch, train_perplexity_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPYIExpaYOek"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4x7nvEeYR0J"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    training_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGrjpJZXgiF"
      },
      "source": [
        "# Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSv0z3IWBRLh"
      },
      "outputs": [],
      "source": [
        "# Create the dataloader for the test set\n",
        "test_dataloader = DataLoader(validation_set, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyjuMrJn79NT"
      },
      "outputs": [],
      "source": [
        "# load modeln\n",
        "model = VQVAE(\n",
        "    HYPER_PARAMS[\"num_hiddens\"],\n",
        "    HYPER_PARAMS[\"num_halving_layers\"],\n",
        "    HYPER_PARAMS[\"num_residual_layers\"],\n",
        "    HYPER_PARAMS[\"num_residual_hiddens\"],\n",
        "    HYPER_PARAMS[\"num_embeddings\"],\n",
        "    HYPER_PARAMS[\"embedding_dim\"],\n",
        "    HYPER_PARAMS[\"commitment_cost\"],\n",
        "    HYPER_PARAMS[\"decay\"]\n",
        ").to(device)\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load(PATH_TO_MODEL_TO_EVALUATE, map_location=torch.device('cuda'))\n",
        ")\n",
        "\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCSqwmaN2JKJ"
      },
      "outputs": [],
      "source": [
        "data_iterator = iter(test_dataloader)\n",
        "batch_number = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvABsS8IPfLd"
      },
      "outputs": [],
      "source": [
        "batch_test_inputs = next(data_iterator).to(device)\n",
        "print(f\"Batch number: {batch_number}\")\n",
        "batch_number += 1\n",
        "test_vq_loss, test_data_recon, test_perplexity = model(batch_test_inputs)\n",
        "batch_data_variance = torch.var(batch_test_inputs, correction=0)\n",
        "print(f\"Test perplexity: {test_perplexity}\")\n",
        "print(f\"Test data_variance: {batch_data_variance}\")\n",
        "print(f\"Test vq_loss: {test_vq_loss}\")\n",
        "print(f\"Test recon_error: {F.mse_loss(test_data_recon, batch_test_inputs) / batch_data_variance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7_-3oOu-QE7n"
      },
      "outputs": [],
      "source": [
        "# @title Play with model results {\"run\":\"auto\",\"vertical-output\":true}\n",
        "num = 0 # @param {\"type\":\"slider\",\"min\":0,\"max\":4,\"step\":1}\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "\n",
        "audio_numpy_v = batch_test_inputs.detach().cpu()[num][0].numpy()\n",
        "audio_numpy_r = test_data_recon.detach().cpu()[num][0].numpy()\n",
        "\n",
        "# Usa IPython.display per riprodurre l'audio\n",
        "ipd.display(ipd.Audio(audio_numpy_v, rate=44100))\n",
        "ipd.display(ipd.Audio(audio_numpy_r, rate=44100))\n",
        "plot_waveform(batch_test_inputs.cpu()[num], graph_title=\"Original waveform\")\n",
        "plot_waveform(test_data_recon.detach().cpu()[num], graph_title=\"Reconstructed waveform\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OdRVdFXVLc3t",
        "KMMLCvIl63zA",
        "95647-9tfSZs",
        "0jSmlnitmvb1",
        "8Ob4w7d2riwU",
        "PeifX3VCsl63",
        "qGwXJnh1jT5k",
        "s0F-8sxCb0-t",
        "wnrU0Vjk9rkt",
        "WAI9PUCecC7b",
        "kWLL6iEoJgsL",
        "b2r0n2fmcj7F",
        "_Ea4LVlZOT_k",
        "Q_jwtl8xmF7E",
        "SRbm1kUumOjS"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "mount_file_id": "1jLm-mAwF6zqYNB8_iuCu0p6qgJuJBHIs",
      "authorship_tag": "ABX9TyMXfQP3A2Lf+PhQ38Tk1wLZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}