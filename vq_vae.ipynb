{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matpaolacci/masked-lm-for-audio/blob/main/vq_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22ptV0pF7AsR"
      },
      "source": [
        "# Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE7NsKI9fXB9"
      },
      "source": [
        "## Set the paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCCiwYjl7DUC"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "# Paths\n",
        "DATASET_HOME_DIR = '/content/drive/MyDrive/DLProj/Dataset'\n",
        "RAW_DATASET_DIR = os.path.join(DATASET_HOME_DIR, \"OriginalVersion\")        # the Slackh2100 dataset\n",
        "WAV_DATASET_DIR = os.path.join(DATASET_HOME_DIR, \"WavVersion\")             # the Slackh2100 dataset with .wav stems\n",
        "UTILITIES_DIR = '/content/drive/MyDrive/DLProj/Utilities'\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/DLProj/Checkpoints'\n",
        "\n",
        "sys.path.append(UTILITIES_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJfkiBYmLTFh"
      },
      "source": [
        "## Set Configuration variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf4dyxBfLZM7"
      },
      "outputs": [],
      "source": [
        "# @title  {\"display-mode\":\"form\"}\n",
        "PREPARE_DATASET = False # @param {\"type\":\"boolean\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBa1VjI0L6k-"
      },
      "outputs": [],
      "source": [
        "# @title Choose if you want to continue to train a pre-saved model and if you want continue the training or only evaluate it {\"form-width\":\"30%\",\"display-mode\":\"form\"}\n",
        "EVALUATE_OR_TRAIN = \"Evaluate model\" # @param [\"Start new training\", \"Evaluate model\",\"Continue training\"]\n",
        "CHECKPOINT_DIRECTORY_NAME = \"2024-09-17_112711\" # @param {\"type\":\"string\",\"placeholder\":\"Enter the directory name 'YYYY-MM-DD_HHMMSS'\"}\n",
        "MODEL_EPOCH_LABEL = 4852 # @param {\"type\":\"integer\"}\n",
        "PRE_SAVED_MODEL_TYPE = \"BEST_RECON_ERR\" # @param [\"BEST_RECON_ERR\",\"BEST_PERPLX\",\"BOTH_PERPLX_RECON_ERR\"]\n",
        "EPOCHS_RE_TRAIN = 3000 # @param {\"type\":\"integer\",\"placeholder\":\"Enter the number of epochs for training\"}\n",
        "\n",
        "# Set following fields only if previous variable has been set to True\n",
        "model_checkpoints_dir = os.path.join(CHECKPOINTS_DIR, CHECKPOINT_DIRECTORY_NAME)\n",
        "PATH_TO_PRE_SAVED_MODEL = os.path.join(model_checkpoints_dir, f'VQVAE_{MODEL_EPOCH_LABEL}_{PRE_SAVED_MODEL_TYPE}')\n",
        "PATH_TO_HYPERPARAMETERS = os.path.join(model_checkpoints_dir, 'hyper_params.json')\n",
        "\n",
        "USE_PRE_SAVED_MODEL = True if EVALUATE_OR_TRAIN == \"Continue training\" else False\n",
        "ONLY_EVALUATION = True if EVALUATE_OR_TRAIN == \"Evaluate model\" else False\n",
        "assert (not USE_PRE_SAVED_MODEL) or (not ONLY_EVALUATION), \"If you want do only evaluation then uncheck 'USE_PRE_SAVED_MODEL'\"\n",
        "\n",
        "EVAL_CHECKPOINT_DIR = os.path.join(CHECKPOINTS_DIR, CHECKPOINT_DIRECTORY_NAME)\n",
        "PATH_TO_MODEL_TO_EVALUATE = os.path.join(EVAL_CHECKPOINT_DIR, f'VQVAE_{MODEL_EPOCH_LABEL}_{PRE_SAVED_MODEL_TYPE}')\n",
        "PATH_TO_HYPERPARAMETERS_EVALUATION = os.path.join(EVAL_CHECKPOINT_DIR, 'hyper_params.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95647-9tfSZs"
      },
      "source": [
        "## Import python libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRhSFMPoe0m-"
      },
      "outputs": [],
      "source": [
        "# Install the required python packages\n",
        "!pip install -r $UTILITIES_DIR/requirements.txt\n",
        "\n",
        "import zipfile, yaml\n",
        "import flacconverter as fc\n",
        "from enum import Enum\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "import gc\n",
        "import json\n",
        "\n",
        "import torch, torchaudio, pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# For getting data structure from list\n",
        "import ast\n",
        "\n",
        "# Libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# To plot the graphs of loss and perplexity\n",
        "from scipy.signal import savgol_filter\n",
        "from matplotlib.ticker import MaxNLocator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jSmlnitmvb1"
      },
      "source": [
        "## Create checkpoints directory\n",
        "It will be created a directory for each training, each one will contain the best trained model across the epochs and a file containing the used hyper parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRYy2eWYnRNn"
      },
      "outputs": [],
      "source": [
        "datetime_start = (datetime.now() + timedelta(hours=2)).strftime(\"%Y-%m-%d_%H%M%S\")\n",
        "CURR_CHECKPOINT_DIR = os.path.join(CHECKPOINTS_DIR, datetime_start)\n",
        "if not ONLY_EVALUATION:\n",
        "    os.makedirs(CURR_CHECKPOINT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ob4w7d2riwU"
      },
      "source": [
        "## Set log file and log function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrUaQfDfrko1"
      },
      "outputs": [],
      "source": [
        "LOG_FILE_PATH = os.path.join(CURR_CHECKPOINT_DIR, 'training.log')\n",
        "\n",
        "def log(message: str):\n",
        "    if ONLY_EVALUATION:\n",
        "        return\n",
        "\n",
        "    with open(LOG_FILE_PATH, \"a\") as log_file:\n",
        "        now = datetime.now()\n",
        "        now_local = now + timedelta(hours=2)\n",
        "        log_file.write(f'[{now_local.strftime(\"%Y-%m-%d %H:%M:%S\")}] - {message}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeifX3VCsl63"
      },
      "source": [
        "# Prepare the data\n",
        "In this section we are going to download the Slakh2100 dataset available [here](http://www.slakh.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfA08phcnqVU"
      },
      "outputs": [],
      "source": [
        "class Instrument(Enum):\n",
        "    BASS = 'Bass'\n",
        "    BRASS = 'Brass'\n",
        "    CHROMATIC_PERCUSSION = 'Chromatic Percussion'\n",
        "    DRUMS = 'Drums'\n",
        "    ETHNIC = 'Ethnic'\n",
        "    GUITAR = 'Guitar'\n",
        "    ORGAN = 'Organ'\n",
        "    PERCUSSIVE = 'Percussive'\n",
        "    PIANO = 'Piano'\n",
        "    PIPE = 'Pipe'\n",
        "    REED = 'Reed'\n",
        "    SOUND_EFFECTS = 'Sound Effects'\n",
        "    STRINGS = 'Strings'\n",
        "    STRINGS_CONTINUED = 'Strings (continued)'\n",
        "    SYNTH_LEAD = 'Synth Lead'\n",
        "    SYNTH_PAD = 'Synth Pad'\n",
        "\n",
        "def extractZipDataset():\n",
        "    # Apri il file ZIP e estrai i contenuti\n",
        "    with zipfile.ZipFile(os.path.join(RAW_DATASET_DIR, 'dataset.zip'), 'r') as zipRef:\n",
        "        zipRef.extractall(RAW_DATASET_DIR)\n",
        "\n",
        "    print(f'File estratti in {RAW_DATASET_DIR}')\n",
        "\n",
        "def convertToWav(baseDir: str, outDir: str):\n",
        "    fc.to_wav(baseDir, outDir, n_threads=2)\n",
        "\n",
        "def createDatasetForVqVae(instruments_set: set[str], split_sets = ['train', 'test', 'validation']):\n",
        "    \"\"\"Questa funzione deve generare un file csv contenente i path, uno per riga, delle tracce audio composte\n",
        "    dagli strumenti nell'instrumentSet.\n",
        "    \"\"\"\n",
        "\n",
        "    for set_dir in split_sets:\n",
        "        csv_stems_file_path = os.path.join(DATASET_HOME_DIR, set_dir + \"_stems.csv\")\n",
        "        csv_mixes_file_path = os.path.join(DATASET_HOME_DIR, set_dir + \"_mixes.csv\")\n",
        "\n",
        "        if os.path.exists(csv_stems_file_path):\n",
        "            print(f\"The file \\'{csv_stems_file_path}\\' already exists! Are you sure you want overwrite it?\")\n",
        "            continue\n",
        "\n",
        "        elif os.path.exists(csv_mixes_file_path):\n",
        "            print(f\"The file \\'{csv_mixes_file_path}\\' already exists! Are you sure you want overwrite it?\")\n",
        "            continue\n",
        "\n",
        "        else:\n",
        "            # Adding the table header to the csv for stems\n",
        "            with open(csv_stems_file_path, 'w') as csv_stems_file:\n",
        "                csv_stems_file.write(\"file_path;instrument_class;midi_program;track_name\\n\")\n",
        "\n",
        "            # Adding the table header to the csv for mixes\n",
        "            with open(csv_mixes_file_path, 'w') as csv_mixes_file:\n",
        "                csv_mixes_file.write(\"file_path;midi_programs;instruments_classes;track_name\\n\")\n",
        "\n",
        "            # Just creating the label to be added to tqdm\n",
        "            tqdm_description = f\"Creating {os.path.basename(csv_stems_file_path)} and {os.path.basename(csv_mixes_file_path)} files\"\n",
        "\n",
        "            # Here we keep track of the file doesn't exist\n",
        "            not_existing_stems = []\n",
        "\n",
        "            for track_dir_name in tqdm(os.listdir(os.path.join(WAV_DATASET_DIR, set_dir)), desc=tqdm_description):\n",
        "                if not os.path.isdir(os.path.join(WAV_DATASET_DIR, set_dir, track_dir_name)):\n",
        "                    continue\n",
        "\n",
        "                # Compose the path to the directory of the track\n",
        "                track_dir_path = os.path.join(WAV_DATASET_DIR, set_dir, track_dir_name)\n",
        "\n",
        "                with open(os.path.join(track_dir_path, \"metadata.yaml\"), 'r') as file:\n",
        "                    yamldata = yaml.safe_load(file)\n",
        "\n",
        "                    # The list of the midi programs that composed the track\n",
        "                    midi_programs_list: list[str] = []\n",
        "\n",
        "                    # The set of the instrument class (es. guitar, piano) that composed the track\n",
        "                    #   we need of a set since a track can be composed by several midi programs\n",
        "                    #   (es. Electic Guitar, Classic Guitar) belonging to the same instrument class\n",
        "                    instruments_classes_set: set[str] = set()\n",
        "\n",
        "                    for stem_name in yamldata[\"stems\"]:\n",
        "                        # get the name of the instruments\n",
        "                        instr_class: str = yamldata[\"stems\"][stem_name][\"inst_class\"]\n",
        "                        midi_program: str = yamldata[\"stems\"][stem_name][\"midi_program_name\"]\n",
        "                        midi_programs_list.append(midi_program)\n",
        "                        instruments_classes_set.add(instr_class)\n",
        "\n",
        "                        if instr_class in instruments_set:\n",
        "\n",
        "                            # The path to the stem file\n",
        "                            stem_file_path = os.path.join(track_dir_path, 'stems', stem_name) + \".wav\"\n",
        "\n",
        "                            # check if the file exists\n",
        "                            if not os.path.exists(stem_file_path):\n",
        "                                not_existing_stems.append(stem_file_path)\n",
        "                                continue\n",
        "\n",
        "                            # add the path to the stem of the intrument to the csv\n",
        "                            entry = f\"{stem_file_path};{instr_class};{midi_program};{track_dir_name.lower()}\\n\"\n",
        "\n",
        "                            with open(csv_stems_file_path, 'a') as csv_stems_file:\n",
        "                                csv_stems_file.write(entry)\n",
        "\n",
        "                    mixes_file_path = os.path.join(track_dir_path, 'mix.wav')\n",
        "\n",
        "                    if not os.path.exists(mixes_file_path):\n",
        "                        not_existing_stems.append(mixes_file_path)\n",
        "                        continue\n",
        "\n",
        "                    with open(csv_mixes_file_path, 'a') as csv_mixes_file:\n",
        "                        entry = f\"{mixes_file_path};{midi_programs_list};{list(instruments_classes_set)};{track_dir_name}\\n\"\n",
        "                        csv_mixes_file.write(entry)\n",
        "\n",
        "            # Print the errors\n",
        "            print(f\"The following files don't exist in the {set_dir} set:\")\n",
        "            for err in not_existing_stems:\n",
        "                print(f\"{' ' * 4}{err}\")\n",
        "\n",
        "\n",
        "def prepareDataset():\n",
        "    extractZipDataset()\n",
        "\n",
        "    # Convert each set to wav\n",
        "    for dir in ['train', 'test', 'validation']:\n",
        "        convertToWav(os.path.join(RAW_DATASET_DIR, dir), os.path.join(WAV_DATASET_DIR, dir))\n",
        "\n",
        "    createDatasetForVqVae({Instrument.BASS.value, Instrument.PIANO.value, Instrument.DRUMS.value, Instrument.GUITAR.value})\n",
        "\n",
        "    print(\"Dataset is ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkG5az2zCbeU"
      },
      "outputs": [],
      "source": [
        "#createDatasetForVqVae({Instrument.BASS.value, Instrument.PIANO.value, Instrument.DRUMS.value, Instrument.GUITAR.value} )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt_FMAUjQvuf"
      },
      "source": [
        "# Look at the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGwXJnh1jT5k"
      },
      "source": [
        "## Composition of the train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2OSkkqIjmHb"
      },
      "outputs": [],
      "source": [
        "FIG_SIZE = (15, 7)\n",
        "\n",
        "pathToHistogramTrain = os.path.join(RAW_DATASET_DIR, \"histograms/train.png\")\n",
        "pathToHistogramTest = os.path.join(RAW_DATASET_DIR, \"histograms/test.png\")\n",
        "pathToHistogramValidation = os.path.join(RAW_DATASET_DIR, \"histograms/validation.png\")\n",
        "\n",
        "trainHist = mpimg.imread(pathToHistogramTrain)\n",
        "plt.figure(figsize=FIG_SIZE)\n",
        "plt.imshow(trainHist)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0F-8sxCb0-t"
      },
      "source": [
        "## Let's Plot the graphic of some tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAohKfpYy0mD"
      },
      "outputs": [],
      "source": [
        "def plot_waveform(waveform, graph_title: str, from_sec:int = None, to_sec: int = None, sample_rate = 44100):\n",
        "\n",
        "    num_channels, num_frames = waveform.shape\n",
        "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "    # Crea il plot\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    for i in range(num_channels):\n",
        "        plt.plot(time_axis.numpy(), waveform[i].numpy(), label=f'Channel {i+1}')\n",
        "\n",
        "    # Imposta range di visualizzazione su asse x\n",
        "    duration_in_sec = num_frames / sample_rate\n",
        "    plt.xlim(from_sec if from_sec is not None else 0, to_sec if to_sec is not None else duration_in_sec)\n",
        "\n",
        "    plt.xlabel('Time (s)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.title(graph_title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnrU0Vjk9rkt"
      },
      "source": [
        "# Define the Pytorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3zQkXV3eSd2"
      },
      "outputs": [],
      "source": [
        "# The header of the csv are\n",
        "#   *_stems.csv -> ['file_path', 'instrument_class', 'midi_program', 'track_name']\n",
        "#   *_mixes.csv -> ['file_path', 'midi_programs', 'instruments_classes', 'track_name']\n",
        "\n",
        "#TODO: da sistemare\n",
        "SAMPLE_RATE = 44100 #Hz\n",
        "\n",
        "def get_alert_message(path, audio_length):\n",
        "    return f\"The file \\\"{path}\\\" is shorter than {audio_length}sec\"\n",
        "\n",
        "class StemsDataset(Dataset):\n",
        "    def __init__(self, csv_audio_path, audio_length, custom_size=None, batch_size=None, transform=None):\n",
        "        self.audio_csv = pd.read_csv(csv_audio_path, delimiter=';')\n",
        "        if(custom_size != None):\n",
        "            sample_size = max(1, min(len(self.audio_csv), custom_size))\n",
        "            self.audio_csv = self.audio_csv.sample(n=sample_size, random_state=1)\n",
        "\n",
        "        self.audio_length = audio_length\n",
        "        self.transform = transform\n",
        "\n",
        "        # In this way we ensure the last batch have the right size\n",
        "        if(batch_size is not None):\n",
        "            assert batch_size <= len(self.audio_csv)\n",
        "            self.audio_csv = self.audio_csv.head(len(self.audio_csv) - (len(self.audio_csv) % batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_csv.iloc[idx, 0]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        instrument_class = self.audio_csv.iloc[idx, 1]\n",
        "        midi_program = self.audio_csv.iloc[idx, 2]\n",
        "\n",
        "        assert waveform.shape[1] >= SAMPLE_RATE * self.audio_length, get_alert_message(audio_path, self.audio_length)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        assert waveform.shape[1]/SAMPLE_RATE >= self.audio_length, f\"The transformed audio is too short ({waveform.shape[1]/SAMPLE_RATE}sec)!\"\n",
        "\n",
        "        # take the first audio_length seconds\n",
        "        waveform = waveform[:,0: SAMPLE_RATE * self.audio_length]\n",
        "\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kztVvJKTw_V6"
      },
      "outputs": [],
      "source": [
        "class MixesDataset(Dataset):\n",
        "    def __init__(self, csv_audio_path, audio_length, custom_size=None, batch_size=None, transform=None):\n",
        "        self.audio_csv = pd.read_csv(csv_audio_path, delimiter=';')\n",
        "        if(custom_size != None):\n",
        "            sample_size = max(1, min(len(self.audio_csv), custom_size))\n",
        "            self.audio_csv = self.audio_csv.sample(n=sample_size, random_state=1)\n",
        "\n",
        "        self.audio_length = audio_length\n",
        "        self.transform = transform\n",
        "\n",
        "        # In this way we ensure the last batch have the right size\n",
        "        if(batch_size is not None):\n",
        "            assert batch_size <= len(self.audio_csv)\n",
        "            self.audio_csv = self.audio_csv.head(len(self.audio_csv) - (len(self.audio_csv) % batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.audio_csv.iloc[idx, 0]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        midi_programs = ast.literal_eval(self.audio_csv.iloc[idx, 1])\n",
        "        instruments_class = ast.literal_eval(self.audio_csv.iloc[idx, 2])\n",
        "        track_name = self.audio_csv.iloc[idx, 3]\n",
        "\n",
        "        assert waveform.shape[1] >= SAMPLE_RATE * self.audio_length, get_alert_message(audio_path, self.audio_length)\n",
        "\n",
        "        if self.transform:\n",
        "            waveform = self.transform(waveform)\n",
        "\n",
        "        assert waveform.shape[1]/SAMPLE_RATE >= self.audio_length, f\"The transformed audio is too short ({waveform.shape[1]/SAMPLE_RATE}sec)!\"\n",
        "\n",
        "        # take the first audio_length seconds\n",
        "        waveform = waveform[:,0: SAMPLE_RATE * self.audio_length]\n",
        "\n",
        "        return waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr1KBGB9gydH"
      },
      "outputs": [],
      "source": [
        "class SingleTrackTestDataset(Dataset):\n",
        "    def __init__(self, csv_audio_path, index_of_the_track, model_input_length, model_batch_size):\n",
        "        audio_csv = pd.read_csv(csv_audio_path, delimiter=';')\n",
        "\n",
        "        assert len(audio_csv) > index_of_the_track, f\"The provided index {{{index_of_the_track}}} is greater than the number of tracks in the dataset {{{len(self.audio_csv)}}}!\"\n",
        "\n",
        "        audio_path = audio_csv.iloc[index_of_the_track, 0]\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        self.track_name = audio_csv.iloc[index_of_the_track, 3]\n",
        "\n",
        "        # Calculate the padding for the track so that we can create the batches to feed the model\n",
        "        chunk_length = SAMPLE_RATE * model_input_length * model_batch_size\n",
        "        remainder = waveform.shape[1] % chunk_length\n",
        "        self.padding_elements_to_add_to_end = chunk_length - remainder\n",
        "\n",
        "        # resize the waveform to adapt it to the audio_lenght set for the model and batch_size for the model\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, self.padding_elements_to_add_to_end))\n",
        "        n_waveforms = waveform.shape[1]//(SAMPLE_RATE * model_input_length)\n",
        "\n",
        "        # It contains n (divisible by batch_size) chunk of the selected track\n",
        "        self.single_track_dataset = waveform.view(n_waveforms, (SAMPLE_RATE * model_input_length))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.single_track_dataset.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.single_track_dataset[idx].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZgl8MDuUm2"
      },
      "source": [
        "# VQ-VAE: The model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4y1dg5ycN9N"
      },
      "source": [
        "## The Vector Quantizer module\n",
        "We calculate the loss by summing up two terms:\n",
        "- **codebook loss**, which moves the embedding towards the encoder output;\n",
        "- **commitment loss**, which makes sure the encoder commits to an embedding;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vnH_2r3NCPm"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim) # TODO: Verifica con float64\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCW -> BWC\n",
        "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # # Calculate L2-normalized distance between the inputs and the codes\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)                  # a^2\n",
        "                    + torch.sum(self._embedding.weight.t()**2, dim=0, keepdim=True) # b^2\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))     # -2ab\n",
        "\n",
        "        # Get the index of the neareast code (from codebook) for each embeddings output by encoder\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = self._embedding(encoding_indices).view(input_shape)\n",
        "\n",
        "        # Calculating commitment loss and codebook loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # commitment loss\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach()) # codebook loss\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator trick for gradient backpropagation\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        # Create a one-hot matrix where the ones at (i, j) position\n",
        "        #   indicates that the j-th code, from codebook, it was selected\n",
        "        #   for the i-th embedding vector output by the encoder.\n",
        "        one_hot_enc = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        one_hot_enc.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Calculates the average probability of the utilization of each code from codebook\n",
        "        avg_probs = torch.mean(one_hot_enc, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BWC -> BCHW\n",
        "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity, quantized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIRG93RwJshe"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        self._commitment_cost = commitment_cost\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim) # TODO: Verifica con float64\n",
        "        self._embedding.weight.data.normal_()\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCW -> BWC\n",
        "        inputs = inputs.permute(0, 2, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # # Calculate L2-normalized distance between the inputs and the codes\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)                  # a^2\n",
        "                    + torch.sum(self._embedding.weight.t()**2, dim=0, keepdim=True) # b^2\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))     # -2ab\n",
        "\n",
        "        # Get the index of the neareast code (from codebook) for each embeddings output by encoder\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = self._embedding(encoding_indices).view(input_shape)\n",
        "\n",
        "        # Create a one-hot matrix where the ones at (i, j) position\n",
        "        #   indicates that the j-th code, from codebook, it was selected\n",
        "        #   for the i-th embedding vector output by the encoder.\n",
        "        one_hot_enc = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        one_hot_enc.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Use EMA to update the embedding vectors (codebook)\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(one_hot_enc, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(one_hot_enc.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Calculating commitment loss and codebook loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs) # commitment loss\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight-through estimator trick for gradient backpropagation\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "\n",
        "        # Calculates the average probability of the utilization of each code from codebook\n",
        "        avg_probs = torch.mean(one_hot_enc, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BWC -> BCHW\n",
        "        return loss, quantized.permute(0, 2, 1).contiguous(), perplexity, quantized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAI9PUCecC7b"
      },
      "source": [
        "## The ResNet module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f7-9JkMb_1j"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv1d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWLL6iEoJgsL"
      },
      "source": [
        "## Function for initializing convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRyQmEzUJlXZ"
      },
      "outputs": [],
      "source": [
        "from math import log2\n",
        "\n",
        "def get_levels(encoder_output_embeddings, num_layers, reverse=False, model_input_sequence_length=None, padding=1, kernel_size=4, stride=2):\n",
        "    '''This function returns a list of 2-ple ((input_channels, output_channels), out_padding_boolean)\n",
        "    within which the first element is a 2-ple representing the input and output dimensions of each encoder/decoder block;\n",
        "    whereas the second element is a boolean which indicates whether or not the output_padding should be added in the\n",
        "    decoded layer which it refers.\n",
        "    Each two subsequent layer it will be a halving/doubling of the dimensionality of the output channels.\n",
        "\n",
        "    For example, for encoder_output_embeddings=128 and num_layers=4 and model_input_sequence_length=132\n",
        "    it will return:\n",
        "            ( (1, 16),   False )\n",
        "            ( (16, 32),  False )\n",
        "            ( (32, 64),  True )\n",
        "            ( (64, 128), False )\n",
        "    '''\n",
        "    input_hiddens = []\n",
        "    output_hiddens = []\n",
        "    output_padding = []\n",
        "\n",
        "    assert log2(encoder_output_embeddings) - (int(log2(encoder_output_embeddings))) == 0\n",
        "    assert log2(encoder_output_embeddings) >= num_layers\n",
        "    assert model_input_sequence_length is None or model_input_sequence_length >= encoder_output_embeddings\n",
        "\n",
        "    # assert if reverse=True than model_input_sequence_length must be provided\n",
        "    assert (not reverse) or model_input_sequence_length is not None\n",
        "\n",
        "    while(encoder_output_embeddings>=2 and num_layers>0):\n",
        "        last = not (encoder_output_embeddings//2>=2 and num_layers-1>0)\n",
        "        input_hiddens.append(encoder_output_embeddings//2 if not last else 1)\n",
        "        output_hiddens.append(encoder_output_embeddings)\n",
        "\n",
        "        # if reverse then calculates if the padding for transposeConv1d is needed\n",
        "        if reverse:\n",
        "            out_sequence_length = \\\n",
        "                (float(model_input_sequence_length + (2 * padding) - (kernel_size - 1) - 1) / stride) + 1\n",
        "            output_padding.append(out_sequence_length - int(out_sequence_length) != 0)\n",
        "            model_input_sequence_length = int(out_sequence_length)\n",
        "\n",
        "        # Update the loop parameters\n",
        "        encoder_output_embeddings = encoder_output_embeddings//2\n",
        "        num_layers-=1\n",
        "\n",
        "    input_hiddens = input_hiddens[::-1 if not reverse else None]\n",
        "    output_hiddens = output_hiddens[::-1 if not reverse else None]\n",
        "\n",
        "    output_padding = output_padding[::-1]\n",
        "\n",
        "    if reverse:\n",
        "        return zip(output_hiddens, input_hiddens, output_padding)\n",
        "    else:\n",
        "        return zip(input_hiddens, output_hiddens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muXre5Tkg6G7"
      },
      "source": [
        "## The Encoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UM4HenS-bZ8q"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation_function):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        self._activation_function = activation_function\n",
        "        self._conv = nn.Conv1d( in_channels=in_channels,\n",
        "                                out_channels=out_channels,\n",
        "                                kernel_size=4,\n",
        "                                stride=2,\n",
        "                                padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._activation_function(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 num_hiddens, # the dimensionality of the embeddings output by the encoder.\n",
        "                 num_halving_layers,\n",
        "                 num_residual_layers,\n",
        "                 num_residual_hiddens):\n",
        "        '''The encoder takes a in_channels-dimensional embedding sequence of length N as input\n",
        "        and maps it in a sequence N/\"2^num_halving_layers\" smaller of embeddings of dimension \"num_hiddens\".\n",
        "        '''\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self._block_activation_function = nn.ReLU()\n",
        "\n",
        "        self._layers = [\n",
        "            EncoderBlock(in_channels=_in_channels,\n",
        "                         out_channels=_out_channels,\n",
        "                         activation_function=nn.Identity() if (_out_channels == num_hiddens) else self._block_activation_function\n",
        "            )\n",
        "            for _in_channels, _out_channels in get_levels(num_hiddens, num_halving_layers)\n",
        "        ] + [\n",
        "            nn.Conv1d(  in_channels=num_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "\n",
        "            ResidualStack(  in_channels=num_hiddens,\n",
        "                            num_hiddens=num_hiddens,\n",
        "                            num_residual_layers=num_residual_layers,\n",
        "                            num_residual_hiddens=num_residual_hiddens),\n",
        "        ]\n",
        "\n",
        "        self._encoder = nn.Sequential(*self._layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self._encoder(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2r0n2fmcj7F"
      },
      "source": [
        "## The Decoder architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmOdjEuobXMG"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, activation_function, output_padding):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self._conv = nn.ConvTranspose1d(in_channels=in_channels,\n",
        "                                        out_channels=out_channels,\n",
        "                                        kernel_size=4,\n",
        "                                        stride=2,\n",
        "                                        padding=1,\n",
        "                                        output_padding=output_padding)\n",
        "\n",
        "        self._activation_function = activation_function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._conv(x)\n",
        "        x = self._activation_function(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 model_input_sequence_length,\n",
        "                 num_hiddens, # the dimensionality of the embeddings output by the encoder.\n",
        "                 num_doubling_layer,\n",
        "                 num_residual_layers,\n",
        "                 num_residual_hiddens):\n",
        "        '''The decoder takes a in_channels-dimensional embedding sequence of length N as input\n",
        "        and maps it in a sequence N*\"2^num_doubling_layer\" longer of embeddings of dimension \"num_hiddens\".\n",
        "        '''\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._block_activation_function = nn.ReLU()\n",
        "        self._output_activation_func = nn.Tanh()  # Output values in range [-1, 1]\n",
        "\n",
        "        self._layers = [\n",
        "            nn.Conv1d(  in_channels=num_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        stride=1,\n",
        "                        padding=1),\n",
        "\n",
        "            ResidualStack(in_channels=num_hiddens,\n",
        "                        num_hiddens=num_hiddens,\n",
        "                        num_residual_layers=num_residual_layers,\n",
        "                        num_residual_hiddens=num_residual_hiddens)\n",
        "        ] + [\n",
        "            DecoderBlock(in_channels=_in_channels,\n",
        "                        out_channels=_out_channels,\n",
        "                        activation_function=nn.Identity() if (_out_channels == 1) else self._block_activation_function,\n",
        "                        output_padding=1 if _output_padding_needed else 0)\n",
        "\n",
        "            for _in_channels, _out_channels, _output_padding_needed in \\\n",
        "                get_levels(\n",
        "                    num_hiddens,\n",
        "                    num_doubling_layer,\n",
        "                    reverse=True,\n",
        "                    model_input_sequence_length=model_input_sequence_length\n",
        "                )\n",
        "        ]\n",
        "\n",
        "        self._decoder = nn.Sequential(*self._layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._decoder(x)\n",
        "        return self._output_activation_func(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8ih6i1JiXfy"
      },
      "source": [
        "## The VQ-VAE model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0YzSnOLibvH"
      },
      "outputs": [],
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_input_sequence_length,\n",
        "                 num_hiddens, # the dimensionality of the embeddings output by the encoder.\n",
        "                 num_halving_layers,\n",
        "                 num_residual_layers,\n",
        "                 num_residual_hiddens,\n",
        "                 num_embeddings,\n",
        "                 embedding_dim,\n",
        "                 commitment_cost,\n",
        "                 decay):\n",
        "\n",
        "        super(VQVAE, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(1,\n",
        "                                num_hiddens,\n",
        "                                num_halving_layers,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "        # It maps the \"num_hiddens-dimensional\" embedding sequence in a \"embedding_dim-dimensional\" sequence\n",
        "        #   preparing it for quantization\n",
        "        self._pre_vq_conv = nn.Conv1d(in_channels=num_hiddens,\n",
        "                                      out_channels=embedding_dim,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1)\n",
        "\n",
        "        if decay > 0.0:\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                               commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
        "                                            commitment_cost)\n",
        "\n",
        "        # It maps the \"embedding_dim-dimensional\" embedding sequence in a \"num_hiddens-dimensional\" embeddin sequence\n",
        "        #   preparing it for the decoding operation (added by me)\n",
        "        self._post_vq_conv = nn.Conv1d(in_channels=embedding_dim,\n",
        "                                       out_channels=num_hiddens,\n",
        "                                       kernel_size=1,\n",
        "                                       stride=1)\n",
        "\n",
        "        self._decoder = Decoder(num_hiddens,\n",
        "                                model_input_sequence_length,\n",
        "                                num_hiddens,\n",
        "                                num_halving_layers,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x)\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        decoder_input = self._post_vq_conv(quantized)\n",
        "        x_recon = self._decoder(decoder_input)\n",
        "\n",
        "        return loss, x_recon, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k_rJNWZqyVZ"
      },
      "source": [
        "# Training of the VQ-VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ea4LVlZOT_k"
      },
      "source": [
        "## Select the device (cpu or gpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zU4dGQO7hyFH"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"You are using {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CO8vSPBqz90"
      },
      "source": [
        "## Define the hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nwcai9uq29e"
      },
      "outputs": [],
      "source": [
        "if USE_PRE_SAVED_MODEL:\n",
        "    with open(PATH_TO_HYPERPARAMETERS, 'r') as hyper_params_file:\n",
        "        HYPER_PARAMS = json.load(hyper_params_file)\n",
        "        HYPER_PARAMS[\"epochs\"] = EPOCHS_RE_TRAIN\n",
        "\n",
        "elif ONLY_EVALUATION:\n",
        "    with open(PATH_TO_HYPERPARAMETERS_EVALUATION, 'r') as hyper_params_file:\n",
        "        HYPER_PARAMS = json.load(hyper_params_file)\n",
        "else:\n",
        "    HYPER_PARAMS = {\n",
        "        \"batch_size\": 4,\n",
        "        \"epochs\": 8000,\n",
        "\n",
        "        # Dataset settings\n",
        "        \"train_size\": 16,\n",
        "        \"validation_size\": 4,\n",
        "        \"test_size\": 4,\n",
        "\n",
        "        # Inputs audio track length in seconds\n",
        "        \"audio_length\": 14,\n",
        "\n",
        "        # Control the silence in the track\n",
        "        \"remove_silence\": True,\n",
        "\n",
        "        # Dimensionality of each encoded vector just before quantization\n",
        "        \"num_hiddens\": 128,\n",
        "\n",
        "        # The quantized sequence has length:\n",
        "        #   ~(audio_length * 44100 / 2^num_halving_layers)\n",
        "        \"num_halving_layers\": 4,\n",
        "\n",
        "        # The ResNet parameters\n",
        "        \"num_residual_hiddens\": 64,\n",
        "        \"num_residual_layers\": 3,\n",
        "\n",
        "        # The dimensionality of the space where the codebook lies\n",
        "        \"embedding_dim\": 64,\n",
        "\n",
        "        # Number of the codes in the codebook\n",
        "        \"num_embeddings\": 512,\n",
        "\n",
        "        # To make sure the encoder commits to an embedding\n",
        "        \"commitment_cost\": 0.25,\n",
        "        \"decay\": 0.99,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"use_spectral\": True\n",
        "    }\n",
        "\n",
        "# Save the hyper params configuration in the training dir\n",
        "if not ONLY_EVALUATION:\n",
        "    with open(os.path.join(CURR_CHECKPOINT_DIR, \"hyper_params.json\"), 'w') as hyp_params_file:\n",
        "        json.dump(HYPER_PARAMS, hyp_params_file, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H0rLSXKDzvi"
      },
      "source": [
        "## Create the Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_jwtl8xmF7E"
      },
      "source": [
        "### Define dataset transformation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-dPKsbimFZD"
      },
      "outputs": [],
      "source": [
        "def trim_silence(waveform: torch.Tensor):\n",
        "    return waveform[waveform != 0].unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRbm1kUumOjS"
      },
      "source": [
        "### Build the Pytorch dataset object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FiDaYxjD4O-"
      },
      "outputs": [],
      "source": [
        "# @title Select what dataset you want to use {\"form-width\":\"20%\",\"display-mode\":\"form\"}\n",
        "USE_STEMS = False # @param {\"type\":\"boolean\"}\n",
        "if USE_STEMS:\n",
        "    train_csv_path = os.path.join(DATASET_HOME_DIR, \"train_stems.csv\")\n",
        "    test_csv_path = os.path.join(DATASET_HOME_DIR, \"test_stems.csv\")\n",
        "    validation_csv_path = os.path.join(DATASET_HOME_DIR, \"validation_stems.csv\")\n",
        "\n",
        "    train_set = StemsDataset(\n",
        "                    train_csv_path,\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    batch_size=HYPER_PARAMS['batch_size'],\n",
        "                    transform=trim_silence if HYPER_PARAMS['remove_silence'] else None\n",
        "    )\n",
        "    test_set = StemsDataset(\n",
        "                    test_csv_path,\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "    )\n",
        "    validation_set = StemsDataset(\n",
        "                    validation_csv_path,\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    batch_size=HYPER_PARAMS['batch_size']\n",
        "    )\n",
        "else:\n",
        "    train_csv_path = os.path.join(DATASET_HOME_DIR, \"train_mixes.csv\")\n",
        "    test_csv_path = os.path.join(DATASET_HOME_DIR, \"test_mixes.csv\")\n",
        "    validation_csv_path = os.path.join(DATASET_HOME_DIR, \"validation_mixes.csv\")\n",
        "\n",
        "    train_set = MixesDataset(\n",
        "                    train_csv_path,\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    custom_size=HYPER_PARAMS['train_size'],\n",
        "                    transform=trim_silence if HYPER_PARAMS['remove_silence'] else None # It was set later than the best training\n",
        "    )\n",
        "    test_set = MixesDataset(\n",
        "                    test_csv_path,\n",
        "                    HYPER_PARAMS['audio_length']\n",
        "    )\n",
        "    validation_set = MixesDataset(\n",
        "                    validation_csv_path,\n",
        "                    HYPER_PARAMS['audio_length'],\n",
        "                    custom_size=HYPER_PARAMS['validation_size']\n",
        "    )\n",
        "\n",
        "train_csv_entries = !wc -l $train_csv_path\n",
        "train_csv_entries = int(train_csv_entries[0].split()[0])\n",
        "test_csv_entries = !wc -l $test_csv_path\n",
        "test_csv_entries = int(test_csv_entries[0].split()[0])\n",
        "validation_csv_entries = !wc -l $validation_csv_path\n",
        "validation_csv_entries = int(validation_csv_entries[0].split()[0])\n",
        "\n",
        "print(f\"------- Original datasets statistics -------\\n\")\n",
        "print(f\"The train csv file has {{{train_csv_entries}}} entries.\")\n",
        "print(f\"The test csv file has {{{test_csv_entries}}} entries.\")\n",
        "print(f\"The validation csv file has {{{validation_csv_entries}}}.\")\n",
        "\n",
        "print(f\"\\n\\n------- Built datasets statistics -------\\n\")\n",
        "print(f\"Train set has {{{len(train_set)}}} entries.\")\n",
        "print(f\"Test set has {{{len(test_set)}}} entries.\")\n",
        "print(f\"Validation set has {{{len(validation_set)}}} entries.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3A1KlF-tbh"
      },
      "source": [
        "## Create the Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra9-Cqux-v0n"
      },
      "outputs": [],
      "source": [
        "train_vqvae_dataloader = DataLoader(train_set, batch_size=HYPER_PARAMS['batch_size'], shuffle=True)\n",
        "validation_vqvae_dataloader = DataLoader(validation_set, batch_size=HYPER_PARAMS['batch_size'], shuffle=True)\n",
        "test_vqvae_dataloader = DataLoader(test_set, batch_size=HYPER_PARAMS[\"batch_size\"], shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVAFttGfNOVB"
      },
      "source": [
        "## Create the model to train\n",
        "If we set the 'USE_PRE_SAVED_MODEL' to *True* then the selected pre-saved model will be train, else a new one will be created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aROGanJrNUTE"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    # define the model\n",
        "    model = VQVAE(\n",
        "        HYPER_PARAMS[\"audio_length\"] * SAMPLE_RATE,\n",
        "        HYPER_PARAMS[\"num_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_halving_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_embeddings\"],\n",
        "        HYPER_PARAMS[\"embedding_dim\"],\n",
        "        HYPER_PARAMS[\"commitment_cost\"],\n",
        "        HYPER_PARAMS[\"decay\"]\n",
        "    ).to(device)\n",
        "\n",
        "    if USE_PRE_SAVED_MODEL:\n",
        "        model.load_state_dict(\n",
        "            torch.load(PATH_TO_PRE_SAVED_MODEL, map_location=torch.device('cuda'))\n",
        "        )\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=HYPER_PARAMS['learning_rate'], amsgrad=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMZZFRYMUBLQ"
      },
      "source": [
        "## Pretty print of the Encoder and Decoder configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "BYm3_cklGaSC"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "model_input_sequence_length = SAMPLE_RATE*HYPER_PARAMS[\"audio_length\"]\n",
        "\n",
        "# calculates output sequence length from convolution\n",
        "conv_out_sequence_len = lambda in_dim: int((float(id_dim + (2 * 1) - (4 - 1) - 1) / 2) + 1)\n",
        "\n",
        "conv_layers = HYPER_PARAMS[\"num_halving_layers\"]\n",
        "lay_i=1\n",
        "encoder_output_embeddings = HYPER_PARAMS[\"num_hiddens\"]\n",
        "\n",
        "print(f\"\\nInfo: The sequence output by the Encoder is {2**conv_layers} times shorter than the input sequence to the model.\\n\")\n",
        "print(\"+--------------------- Display Encoder layers setting ---------------------+\\n\")\n",
        "\n",
        "enc_table = PrettyTable()\n",
        "enc_table.field_names = [\"input_embeddings\", \"out_embeddings\", \"out_sequence_length\"]\n",
        "\n",
        "for i, o in get_levels(encoder_output_embeddings, conv_layers, reverse=False):\n",
        "    enc_table.add_row((i, o, int(model_input_sequence_length/2**(lay_i))))\n",
        "    lay_i+=1\n",
        "\n",
        "print(enc_table)\n",
        "\n",
        "print(\"\\n\\n+--------------------- Display Decoder layers setting ---------------------+\\n\")\n",
        "\n",
        "dec_table = PrettyTable()\n",
        "dec_table.field_names = [\"input_embeddings\", \"out_embeddings\", \"output_padding\", \"out_sequence_length\"]\n",
        "\n",
        "lay_i=0 #reset\n",
        "for i, o, pad in get_levels(encoder_output_embeddings, conv_layers, reverse=True, model_input_sequence_length=model_input_sequence_length):\n",
        "    dec_table.add_row((i, o, pad, int(model_input_sequence_length/2**(conv_layers-lay_i))))\n",
        "    lay_i+=1\n",
        "\n",
        "print(dec_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMwuQUjmoELC"
      },
      "source": [
        "## Define the Spectral Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOqRbljgoJBL"
      },
      "outputs": [],
      "source": [
        "def compute_stft_magnitude(x):\n",
        "    stft_x = torch.stft(x, n_fft=1024, hop_length=256, win_length=1024, window=torch.hann_window(1024, device=device), return_complex=True)\n",
        "    return torch.abs(stft_x)\n",
        "\n",
        "def spectral_loss(x_original, x_reconstructed):\n",
        "    '''Compute the magnitude of the STFT for both signals'''\n",
        "    mag_x_original = compute_stft_magnitude(x_original.squeeze(1)).unsqueeze(1)\n",
        "    mag_x_reconstructed = compute_stft_magnitude(x_reconstructed.squeeze(1)).unsqueeze(1)\n",
        "\n",
        "    # Return the loss as L2 between signals\n",
        "    return F.mse_loss(mag_x_original, mag_x_reconstructed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQaCaYz4qr7"
      },
      "source": [
        "## Define a function that plots training statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_yKjgsWezdz"
      },
      "outputs": [],
      "source": [
        "def plot_loss_and_perplexity(train_res_recon_error, train_perplexity, valid_recon_error, valid_perplexity):\n",
        "    WINDOW_LENGTH = 201\n",
        "\n",
        "    # Create a figure with a 2x2 grid of graphs\n",
        "    f = plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Graph 1: NMSE train\n",
        "    ax = f.add_subplot(2, 2, 1)\n",
        "    ax.plot(savgol_filter(train_res_recon_error, WINDOW_LENGTH, 7))\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_title('Smoothed NMSE (Train)')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    # Graph 2: Perplexity train\n",
        "    ax = f.add_subplot(2, 2, 2)\n",
        "    ax.plot(savgol_filter(train_perplexity, WINDOW_LENGTH, 7))\n",
        "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
        "    ax.set_title('Train Smoothed Average codebook usage (Perplexity)')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    # Graph 3: NMSE validation\n",
        "    ax = f.add_subplot(2, 2, 3)\n",
        "    ax.plot(savgol_filter(valid_recon_error, WINDOW_LENGTH, 7))\n",
        "    ax.set_yscale('log')\n",
        "    ax.set_title('Smoothed NMSE (Validation)')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    # Graph 4: Perplexity validation\n",
        "    ax = f.add_subplot(2, 2, 4)\n",
        "    ax.plot(savgol_filter(valid_perplexity, WINDOW_LENGTH, 7))\n",
        "    ax.yaxis.set_major_locator(MaxNLocator(nbins=10))\n",
        "    ax.set_title('Validation Smoothed Average codebook usage (Perplexity)')\n",
        "    ax.set_xlabel('epochs')\n",
        "\n",
        "    f.savefig(os.path.join(CURR_CHECKPOINT_DIR, \"train_statistics.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYPuEYXN_Ne8"
      },
      "source": [
        "## Define a per-epoch training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGf7Gsz4burt"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(train_dataloader: DataLoader, epoch_index):\n",
        "\n",
        "    train_res_recon_error = []\n",
        "    train_res_perplexity = []\n",
        "\n",
        "    for _, inputs in enumerate(train_dataloader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        data_variance = torch.var(inputs, correction=0)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        vq_loss, data_recon, perplexity = model(inputs)\n",
        "\n",
        "        # Compute the losses and its gradients\n",
        "        recon_error = F.mse_loss(data_recon, inputs) / data_variance\n",
        "\n",
        "        if HYPER_PARAMS[\"use_spectral\"]:\n",
        "            # Compute the Spectral Loss\n",
        "            spectral_loss_value = spectral_loss(inputs, data_recon)\n",
        "        else:\n",
        "            spectral_loss_value = 0\n",
        "\n",
        "        #vq_loss = vq_loss / data_variance\n",
        "        loss = recon_error + vq_loss + spectral_loss_value\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        train_res_recon_error.append(recon_error.item())\n",
        "        train_res_perplexity.append(perplexity.item())\n",
        "\n",
        "    return np.mean(train_res_recon_error), np.mean(train_res_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpeRM4-A_Xt2"
      },
      "source": [
        "## Define the main training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcgDp1iDbt07"
      },
      "outputs": [],
      "source": [
        "KEEP_LAST_N_MODELS = 1\n",
        "THRESHOLD_ERROR_TO_SAVE_MODEL = 0.9\n",
        "\n",
        "def training_loop():\n",
        "\n",
        "    epoch_number = 1\n",
        "\n",
        "    best_valid_recon_error = 1_000_000\n",
        "    best_valid_perplexity = 1\n",
        "\n",
        "    # To keep track of the saved models\n",
        "    saved_models = {\n",
        "        \"RECON_ERROR\": [],\n",
        "        \"PERPLX\": [],\n",
        "        \"BOTH\": []\n",
        "    }\n",
        "\n",
        "    # To keep track of the loss and perplexity along epochs and finally plot the graphs\n",
        "    train_recon_error_per_epoch = []\n",
        "    train_perplexity_per_epoch = []\n",
        "    valid_recon_error_per_epoch = []\n",
        "    valid_perplexity_per_epoch = []\n",
        "\n",
        "    with tqdm(total=HYPER_PARAMS[\"epochs\"]) as pbar:\n",
        "        for epoch in range(HYPER_PARAMS[\"epochs\"]):\n",
        "            log('EPOCH {}'.format(epoch_number))\n",
        "\n",
        "            # Make sure gradient tracking is on, and do a pass over the data\n",
        "            model.train()\n",
        "            avg_train_recon_error, avg_train_perplexity = train_one_epoch(train_vqvae_dataloader,epoch_number)\n",
        "\n",
        "            # Store statistics\n",
        "            train_recon_error_per_epoch.append(avg_train_recon_error)\n",
        "            train_perplexity_per_epoch.append(avg_train_perplexity)\n",
        "\n",
        "            # Set the model to evaluation mode, disabling dropout and using population\n",
        "            # statistics for batch normalization.\n",
        "            model.eval()\n",
        "\n",
        "            valid_res_recon_error = []\n",
        "            valid_res_perplexity = []\n",
        "\n",
        "            # Disable gradient computation and reduce memory consumption.\n",
        "            with torch.no_grad():\n",
        "                for i, valid_inputs in enumerate(validation_vqvae_dataloader):\n",
        "                    valid_inputs = valid_inputs.to(device)\n",
        "                    data_variance = torch.var(valid_inputs, correction=0)\n",
        "                    valid_vq_loss, valid_data_recon, valid_perplexity = model(valid_inputs)\n",
        "                    recon_error = F.mse_loss(valid_data_recon, valid_inputs) / data_variance\n",
        "                    valid_res_recon_error.append(recon_error.item())\n",
        "                    valid_res_perplexity.append(valid_perplexity.item())\n",
        "\n",
        "            avg_valid_recon_error = np.mean(valid_res_recon_error)\n",
        "            avg_valid_perplexity = np.mean(valid_res_perplexity)\n",
        "\n",
        "            # Store statistics\n",
        "            valid_recon_error_per_epoch.append(avg_valid_recon_error)\n",
        "            valid_perplexity_per_epoch.append(avg_valid_perplexity)\n",
        "\n",
        "            # Log error and perplexity\n",
        "            log(f\"Train reconstruction error: {avg_train_recon_error} - Validation reconstruction error: {avg_valid_recon_error}\")\n",
        "            log(f\"Train perplexity: {avg_train_perplexity} - Validation perplexity: {avg_valid_perplexity}\")\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'Train reconstruction err': f'{avg_train_recon_error}',\n",
        "                'Train perplexity': f'{avg_train_perplexity}'\n",
        "            })\n",
        "\n",
        "            # Check if the error is lower than set threshold\n",
        "            if (avg_valid_recon_error < THRESHOLD_ERROR_TO_SAVE_MODEL):\n",
        "\n",
        "                if (avg_valid_recon_error < best_valid_recon_error or\n",
        "                    avg_valid_perplexity > best_valid_perplexity):\n",
        "\n",
        "                    select_storage = None\n",
        "\n",
        "                    if (avg_valid_recon_error < best_valid_recon_error and avg_valid_perplexity > best_valid_perplexity):\n",
        "                        best_valid_recon_error = avg_valid_recon_error\n",
        "                        best_valid_perplexity = avg_valid_perplexity\n",
        "                        model_name = 'VQVAE_{}_BEST_BOTH_PERPLX_RECON_ERR'.format(epoch_number)\n",
        "                        select_storage = \"BOTH\"\n",
        "\n",
        "                    # Save the model because validation reconstruction error is improved\n",
        "                    elif (avg_valid_recon_error < best_valid_recon_error):\n",
        "                        best_valid_recon_error = avg_valid_recon_error\n",
        "                        model_name = 'VQVAE_{}_BEST_RECON_ERR'.format(epoch_number)\n",
        "                        select_storage = \"RECON_ERROR\"\n",
        "\n",
        "                    # Save the model beacause validation perplexity as grown\n",
        "                    elif (avg_valid_perplexity > best_valid_perplexity):\n",
        "                        best_valid_perplexity = avg_valid_perplexity\n",
        "                        model_name = 'VQVAE_{}_BEST_PERPLX'.format(epoch_number)\n",
        "                        select_storage = \"PERPLX\"\n",
        "\n",
        "                    # Save the model\n",
        "                    model_path = os.path.join(CURR_CHECKPOINT_DIR, model_name)\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "                    # Keep track of saved models\n",
        "                    saved_models[select_storage].append(model_path)\n",
        "\n",
        "                    # delete too old saved models\n",
        "                    if len(saved_models[select_storage]) > KEEP_LAST_N_MODELS:\n",
        "                        os.remove(saved_models[select_storage][0])\n",
        "                        saved_models[select_storage] = saved_models[select_storage][1:]\n",
        "\n",
        "            epoch_number += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "    plot_loss_and_perplexity(train_recon_error_per_epoch, train_perplexity_per_epoch, valid_recon_error_per_epoch, valid_perplexity_per_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPYIExpaYOek"
      },
      "source": [
        "## Train!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4x7nvEeYR0J"
      },
      "outputs": [],
      "source": [
        "if not ONLY_EVALUATION:\n",
        "    training_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGrjpJZXgiF"
      },
      "source": [
        "# Quantitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyjuMrJn79NT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# If we are only evaluating then we must load a pre-trained model\n",
        "#   else we use the newly trained model\n",
        "if ONLY_EVALUATION:\n",
        "    model = VQVAE(\n",
        "        HYPER_PARAMS[\"audio_length\"] * SAMPLE_RATE,\n",
        "        HYPER_PARAMS[\"num_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_halving_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_layers\"],\n",
        "        HYPER_PARAMS[\"num_residual_hiddens\"],\n",
        "        HYPER_PARAMS[\"num_embeddings\"],\n",
        "        HYPER_PARAMS[\"embedding_dim\"],\n",
        "        HYPER_PARAMS[\"commitment_cost\"],\n",
        "        HYPER_PARAMS[\"decay\"]\n",
        "    ).to(device)\n",
        "\n",
        "    model.load_state_dict(\n",
        "        torch.load(\n",
        "            PATH_TO_MODEL_TO_EVALUATE,\n",
        "            map_location=torch.device('cuda')\n",
        "        )\n",
        "    )\n",
        "\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCSqwmaN2JKJ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Select the dataset {\"run\":\"auto\"}\n",
        "dataset_to_evaluate_on = test_vqvae_dataloader # @param [\"train_vqvae_dataloader\",\"validation_vqvae_dataloader\",\"test_vqvae_dataloader\"] {\"type\":\"raw\"}\n",
        "data_iterator = iter(dataset_to_evaluate_on)\n",
        "batch_number = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mvABsS8IPfLd"
      },
      "outputs": [],
      "source": [
        "# @title Re-run this cell if you want to access the next batch\n",
        "\n",
        "batch_test_inputs = next(data_iterator).to(device)\n",
        "print(f\"Batch number: {batch_number}\")\n",
        "batch_number += 1\n",
        "test_vq_loss, test_data_recon, test_perplexity = model(batch_test_inputs)\n",
        "batch_data_variance = torch.var(batch_test_inputs, correction=0)\n",
        "print(f\"Test perplexity: {test_perplexity}\")\n",
        "print(f\"Test data_variance: {batch_data_variance}\")\n",
        "print(f\"Test vq_loss: {test_vq_loss}\")\n",
        "print(f\"Test recon_error: {F.mse_loss(test_data_recon, batch_test_inputs) / batch_data_variance}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_-3oOu-QE7n",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ## Play with Test data {\"run\":\"auto\",\"vertical-output\":true}\n",
        "num = 1 # @param {\"type\":\"slider\",\"min\":0,\"max\":4,\"step\":1}\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "\n",
        "audio_numpy_v = batch_test_inputs.detach().cpu()[num][0].numpy()\n",
        "audio_numpy_r = test_data_recon.detach().cpu()[num][0].numpy()\n",
        "\n",
        "# Usa IPython.display per riprodurre l'audio\n",
        "ipd.display(ipd.Audio(audio_numpy_v, rate=44100))\n",
        "ipd.display(ipd.Audio(audio_numpy_r, rate=44100))\n",
        "plot_waveform(batch_test_inputs.cpu()[num], graph_title=\"Original waveform\")\n",
        "plot_waveform(test_data_recon.detach().cpu()[num], graph_title=\"Reconstructed waveform\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## Reconstruct entire tracks from Train, Validation or Test set {\"form-width\":\"20%\"}\n",
        "\n",
        "def reconstruct_the_track(dataloader, padding_added_to_the_end: int):\n",
        "    batch_test_recon_error = []\n",
        "    batch_test_res_perplexity = []\n",
        "    reconstructed_track = torch.tensor([]).cpu()\n",
        "\n",
        "    for input in iter(dataloader):\n",
        "        input = input.to(device)\n",
        "        test_vq_loss, test_data_recon, test_perplexity = model(input)\n",
        "        batch_data_variance = torch.var(batch_test_inputs, correction=0)\n",
        "        recon_error = F.mse_loss(test_data_recon, batch_test_inputs) / batch_data_variance\n",
        "        batch_test_recon_error.append(recon_error.item())\n",
        "        batch_test_res_perplexity.append(test_perplexity.item())\n",
        "        reconstructed_track = \\\n",
        "            torch.cat((reconstructed_track, test_data_recon.detach().view(HYPER_PARAMS['batch_size'] * SAMPLE_RATE * HYPER_PARAMS['audio_length']).cpu()), dim=0)\n",
        "\n",
        "    reconstructed_track = reconstructed_track[:-padding_added_to_the_end].unsqueeze(0).cpu()\n",
        "\n",
        "    return reconstructed_track, np.mean(batch_test_res_perplexity), np.mean(batch_test_recon_error)\n",
        "\n",
        "\n",
        "def reconstruct_the_dataset(dataset_csv: str, selected_dataset: str, n_tracks_to_reconstruct: int):\n",
        "    working_dir = os.path.join(EVAL_CHECKPOINT_DIR, \"ReconstrutedTracks\", selected_dataset)\n",
        "    assert not os.path.exists(working_dir), f\"The working directory '{working_dir}' already exists!\"\n",
        "    os.makedirs(working_dir, exist_ok=False)\n",
        "\n",
        "    print(f\"Reconstructed audio tracks wirth statistics are saved at '{working_dir}'\")\n",
        "\n",
        "    statistics_file = os.path.join(working_dir, \"Statistics.csv\")\n",
        "    assert not os.path.exists(statistics_file), \"The Statistics file already exists\"\n",
        "\n",
        "    label_for_recon_error = f\"Reconstrucion Error{' (With Spectral loss)' if HYPER_PARAMS['use_spectral'] else None}\"\n",
        "    with open(statistics_file, 'w') as f:\n",
        "        f.write(f\"Track name;{label_for_recon_error};Perplexity;Average {label_for_recon_error};Average Perplexity\\n\")\n",
        "\n",
        "    recon_error_along_dataset = []\n",
        "    perplexity_along_dataset = []\n",
        "\n",
        "    for track_i in tqdm(range(n_tracks_to_reconstruct)):\n",
        "        single_track_test_set = SingleTrackTestDataset(\n",
        "                                    dataset_csv,\n",
        "                                    track_i,\n",
        "                                    HYPER_PARAMS['audio_length'],\n",
        "                                    HYPER_PARAMS['batch_size']\n",
        "                                )\n",
        "\n",
        "        single_track_dataloader = DataLoader(\n",
        "                                    single_track_test_set,\n",
        "                                    batch_size=HYPER_PARAMS['batch_size'],\n",
        "                                    shuffle=False\n",
        "                                )\n",
        "\n",
        "        reconstructed_track, perplexity, recon_error = reconstruct_the_track(\n",
        "                                                            single_track_dataloader,\n",
        "                                                            single_track_test_set.padding_elements_to_add_to_end\n",
        "                                                    )\n",
        "        track_name = single_track_test_set.track_name\n",
        "\n",
        "        # Clean memory\n",
        "        del single_track_dataloader, single_track_test_set\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Save the reconstructed track into file\n",
        "        torchaudio.save(os.path.join(working_dir,f'{track_name}.wav'), reconstructed_track, SAMPLE_RATE)\n",
        "\n",
        "        # Store statistics\n",
        "        recon_error_along_dataset.append(recon_error)\n",
        "        perplexity_along_dataset.append(perplexity)\n",
        "\n",
        "        with open(statistics_file, 'a') as f:\n",
        "            f.write(f\"{track_name};{recon_error};{perplexity};-;-\\n\")\n",
        "\n",
        "    with open(statistics_file, 'a') as f:\n",
        "        f.write(f\"-;-;-;{np.mean(recon_error_along_dataset)};{np.mean(perplexity_along_dataset)}\\n\")\n",
        "\n",
        "# @title  {\"form-width\":\"30%\"}\n",
        "selected_dataset_to_reconstruct = \"Test\" # @param [\"Train\",\"Validation\",\"Test\"]\n",
        "n_tracks_to_reconstruct = 31 # @param {\"type\":\"integer\",\"placeholder\":\"Insert the number of tracks you want to reconstruct from the selected dataset\"}\n",
        "selected_csv = None\n",
        "\n",
        "if selected_dataset_to_reconstruct == \"Train\":\n",
        "    selected_csv = train_csv_path\n",
        "    assert n_tracks_to_reconstruct <= len(train_set), \"There are not so many data in the Train set\"\n",
        "elif selected_dataset_to_reconstruct == \"Validation\":\n",
        "    selected_csv = validation_csv_path\n",
        "    assert n_tracks_to_reconstruct <= len(validation_set), \"There are not so many data in the Train set\"\n",
        "elif selected_dataset_to_reconstruct == \"Test\":\n",
        "    selected_csv = test_csv_path\n",
        "    assert n_tracks_to_reconstruct <= len(test_set), \"There are not so many data in the Train set\"\n",
        "\n",
        "reconstruct_the_dataset(selected_csv, selected_dataset_to_reconstruct, n_tracks_to_reconstruct)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "_HincEFZPBym"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RE7NsKI9fXB9",
        "PeifX3VCsl63",
        "qGwXJnh1jT5k",
        "WAI9PUCecC7b",
        "kWLL6iEoJgsL",
        "muXre5Tkg6G7",
        "b2r0n2fmcj7F",
        "r8ih6i1JiXfy",
        "Q_jwtl8xmF7E",
        "vMwuQUjmoELC"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1jLm-mAwF6zqYNB8_iuCu0p6qgJuJBHIs",
      "authorship_tag": "ABX9TyPZn8kU8BL4o/WAdGa2SfaJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}